{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAveejHceCyg",
        "outputId": "c424293f-930e-4f5f-e27a-8e8c8f727c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-T93r862UxJ",
        "outputId": "641feb86-cfa3-42bf-f5d8-13d38c907ab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NCKH Depression\n",
            " Code\n",
            " Dataset\n",
            "'FIX TMDT.docx'\n",
            "'Giấy tờ'\n",
            "'Khảo sát mức độ trầm cảm (Câu trả lời).gsheet'\n",
            "'Khảo sát mức độ trầm cảm.gform'\n",
            "'Model - EM - Expectation Maximization'\n",
            " Page\n",
            "'Project FINALdocx.docx'\n",
            "'QR Code Form.png'\n",
            "'Quà '\n",
            "'Thuyết Minh NCKH 2022.docx'\n",
            "'Thuyết Minh NCKH 2022 - Tú.docx'\n",
            "'Tucuteee (Được Tự Phục hồi).docx'\n",
            " VNICT\n",
            " Website\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/NCKH Depression'\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOemaND63Iur"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from pandas import read_csv\n",
        "from pandas import set_option\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDQGjYbU3Lbk"
      },
      "source": [
        "# Step 1: Import, preprocess."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DQoS1ozx3K6J",
        "outputId": "cfdda79d-1882-46f7-a605-3e25715e4acd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Họ và tên bạn là gì? Ngày sinh của bạn là? Hiện tại bạn là?  \\\n",
              "0       Phạm Trần Anh Vũ            2003-12-05  Sinh viên năm 1   \n",
              "1    Nguyễn Trung Nguyên            2002-11-09  Sinh viên năm 2   \n",
              "2    Mai Thị Huyền Trang            2003-08-09  Sinh viên năm 1   \n",
              "3         Dương Hồng Mai            2003-03-02  Sinh viên năm 1   \n",
              "4  Nguyễn Đặng Thành Duy            2003-06-17  Sinh viên năm 1   \n",
              "\n",
              "  Giới tính sinh học của bạn là: Bạn có thuộc cộng đồng LGBT không?  \\\n",
              "0                            Nam                                 Có   \n",
              "1                            Nam                              Không   \n",
              "2                             Nữ                              Không   \n",
              "3                             Nữ                              Không   \n",
              "4                            Nam                              Không   \n",
              "\n",
              "    1. Bạn là người có xu hướng:  \\\n",
              "0  Vừa hướng nội vừa hướng ngoại   \n",
              "1             Chưa xác định được   \n",
              "2  Vừa hướng nội vừa hướng ngoại   \n",
              "3                    Hướng ngoại   \n",
              "4  Vừa hướng nội vừa hướng ngoại   \n",
              "\n",
              "  2. Bạn có cảm thấy bản thân mình thừa thãi không?  \\\n",
              "0                                          Một chút   \n",
              "1                                             Không   \n",
              "2                                             Không   \n",
              "3                                          Một chút   \n",
              "4                                          Một chút   \n",
              "\n",
              "  3. Bạn có cảm thấy lạc lõng, không hòa nhập được với mọi người không?  \\\n",
              "0                                           Một chút                      \n",
              "1                                              Không                      \n",
              "2                                              Không                      \n",
              "3                                              Không                      \n",
              "4                                           Một chút                      \n",
              "\n",
              "  4. Bạn đang tự ti? 5. Bạn có hay vận động thể dục, thể thao không?  ...  \\\n",
              "0                 Có                                           Không  ...   \n",
              "1           Một chút                                              Có  ...   \n",
              "2                 Có                                        Một chút  ...   \n",
              "3           Một chút                                        Một chút  ...   \n",
              "4           Một chút                                           Không  ...   \n",
              "\n",
              "  1. Bạn đột nhiên ăn quá nhiều hoặc chán ăn, biếng ăn?  \\\n",
              "0                                                2.0      \n",
              "1                                                3.0      \n",
              "2                                                3.0      \n",
              "3                                                2.0      \n",
              "4                                                1.0      \n",
              "\n",
              "  2. Bạn luôn thấy mệt mỏi, thiếu năng lượng làm mọi việc?  \\\n",
              "0                                                1.0         \n",
              "1                                                2.0         \n",
              "2                                                3.0         \n",
              "3                                                1.0         \n",
              "4                                                0.0         \n",
              "\n",
              "  3. Bạn cảm thấy khó ngủ, ngủ không lâu hoặc ngược lại ngủ quá nhiều?  \\\n",
              "0                                                3.0                     \n",
              "1                                                2.0                     \n",
              "2                                                1.0                     \n",
              "3                                                1.0                     \n",
              "4                                                1.0                     \n",
              "\n",
              "   4. Bạn hoạt động chậm chạp, đi lại hay nói chuyện đều chậm khiến mọi người chú ý. Hoặc bồn chồn không thể ở yên một chỗ?  \\\n",
              "0                                                0.0                                                                          \n",
              "1                                                1.0                                                                          \n",
              "2                                                3.0                                                                          \n",
              "3                                                3.0                                                                          \n",
              "4                                                0.0                                                                          \n",
              "\n",
              "   5. Bạn không thể tập trung khi làm việc?  \\\n",
              "0                                       2.0   \n",
              "1                                       1.0   \n",
              "2                                       3.0   \n",
              "3                                       3.0   \n",
              "4                                       1.0   \n",
              "\n",
              "   6. Bạn mất tự tin vào bản thân, thất vọng về bản thân và cả gia đình?  \\\n",
              "0                                                1.0                       \n",
              "1                                                0.0                       \n",
              "2                                                1.0                       \n",
              "3                                                1.0                       \n",
              "4                                                0.0                       \n",
              "\n",
              "   7. Bạn luôn cảm thấy chán nản, kiệt sức, tuyệt vọng?  \\\n",
              "0                                                1.0      \n",
              "1                                                1.0      \n",
              "2                                                2.0      \n",
              "3                                                1.0      \n",
              "4                                                0.0      \n",
              "\n",
              "   8. Bạn ít thấy hứng thú hoặc không tìm thấy niềm vui trong mọi việc?  \\\n",
              "0                                                2.0                      \n",
              "1                                                1.0                      \n",
              "2                                                1.0                      \n",
              "3                                                1.0                      \n",
              "4                                                0.0                      \n",
              "\n",
              "  9. Bạn có suy nghĩ tiêu cực, muốn làm tổn thương bản thân thậm chí có suy nghĩ tự sát?  \\\n",
              "0                                                1.0                                       \n",
              "1                                                0.0                                       \n",
              "2                                                0.0                                       \n",
              "3                                                0.0                                       \n",
              "4                                                0.0                                       \n",
              "\n",
              "   Tổng điểm  \n",
              "0       13.0  \n",
              "1       11.0  \n",
              "2       17.0  \n",
              "3       13.0  \n",
              "4        3.0  \n",
              "\n",
              "[5 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c882b4c-d82f-41cc-b32c-e28571533dc1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Họ và tên bạn là gì?</th>\n",
              "      <th>Ngày sinh của bạn là?</th>\n",
              "      <th>Hiện tại bạn là?</th>\n",
              "      <th>Giới tính sinh học của bạn là:</th>\n",
              "      <th>Bạn có thuộc cộng đồng LGBT không?</th>\n",
              "      <th>1. Bạn là người có xu hướng:</th>\n",
              "      <th>2. Bạn có cảm thấy bản thân mình thừa thãi không?</th>\n",
              "      <th>3. Bạn có cảm thấy lạc lõng, không hòa nhập được với mọi người không?</th>\n",
              "      <th>4. Bạn đang tự ti?</th>\n",
              "      <th>5. Bạn có hay vận động thể dục, thể thao không?</th>\n",
              "      <th>...</th>\n",
              "      <th>1. Bạn đột nhiên ăn quá nhiều hoặc chán ăn, biếng ăn?</th>\n",
              "      <th>2. Bạn luôn thấy mệt mỏi, thiếu năng lượng làm mọi việc?</th>\n",
              "      <th>3. Bạn cảm thấy khó ngủ, ngủ không lâu hoặc ngược lại ngủ quá nhiều?</th>\n",
              "      <th>4. Bạn hoạt động chậm chạp, đi lại hay nói chuyện đều chậm khiến mọi người chú ý. Hoặc bồn chồn không thể ở yên một chỗ?</th>\n",
              "      <th>5. Bạn không thể tập trung khi làm việc?</th>\n",
              "      <th>6. Bạn mất tự tin vào bản thân, thất vọng về bản thân và cả gia đình?</th>\n",
              "      <th>7. Bạn luôn cảm thấy chán nản, kiệt sức, tuyệt vọng?</th>\n",
              "      <th>8. Bạn ít thấy hứng thú hoặc không tìm thấy niềm vui trong mọi việc?</th>\n",
              "      <th>9. Bạn có suy nghĩ tiêu cực, muốn làm tổn thương bản thân thậm chí có suy nghĩ tự sát?</th>\n",
              "      <th>Tổng điểm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Phạm Trần Anh Vũ</td>\n",
              "      <td>2003-12-05</td>\n",
              "      <td>Sinh viên năm 1</td>\n",
              "      <td>Nam</td>\n",
              "      <td>Có</td>\n",
              "      <td>Vừa hướng nội vừa hướng ngoại</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Có</td>\n",
              "      <td>Không</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Nguyễn Trung Nguyên</td>\n",
              "      <td>2002-11-09</td>\n",
              "      <td>Sinh viên năm 2</td>\n",
              "      <td>Nam</td>\n",
              "      <td>Không</td>\n",
              "      <td>Chưa xác định được</td>\n",
              "      <td>Không</td>\n",
              "      <td>Không</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Có</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Mai Thị Huyền Trang</td>\n",
              "      <td>2003-08-09</td>\n",
              "      <td>Sinh viên năm 1</td>\n",
              "      <td>Nữ</td>\n",
              "      <td>Không</td>\n",
              "      <td>Vừa hướng nội vừa hướng ngoại</td>\n",
              "      <td>Không</td>\n",
              "      <td>Không</td>\n",
              "      <td>Có</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>...</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Dương Hồng Mai</td>\n",
              "      <td>2003-03-02</td>\n",
              "      <td>Sinh viên năm 1</td>\n",
              "      <td>Nữ</td>\n",
              "      <td>Không</td>\n",
              "      <td>Hướng ngoại</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Không</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nguyễn Đặng Thành Duy</td>\n",
              "      <td>2003-06-17</td>\n",
              "      <td>Sinh viên năm 1</td>\n",
              "      <td>Nam</td>\n",
              "      <td>Không</td>\n",
              "      <td>Vừa hướng nội vừa hướng ngoại</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Không</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c882b4c-d82f-41cc-b32c-e28571533dc1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c882b4c-d82f-41cc-b32c-e28571533dc1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c882b4c-d82f-41cc-b32c-e28571533dc1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "data = pd.read_excel('/content/drive/MyDrive/NCKH Depression/Dataset/Education.xlsx')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZVHwT0m3T8f"
      },
      "outputs": [],
      "source": [
        "data.columns = ['Name', 'DoB', 'Job', 'Sex', 'LGBT', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', \n",
        "                'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9', 'b10', 'b11', 'b12', 'b13', 'b14', 'b15',\n",
        "                'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'Total']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxvQIc_j3hrt"
      },
      "source": [
        "$Data$ $Preprocessing$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fswScaZ3Yq8"
      },
      "outputs": [],
      "source": [
        "# Change values in total column to the rate of depression. Normal - Minimal - Mild - Medium - Severe.\n",
        "data.Total = data.Total.replace({1: 0, 2: 0, 3: 0, 4: 0,\n",
        "                                 5: 1, 6: 1, 7: 1, 8: 1, 9: 1,\n",
        "                                 10: 2, 11: 2, 12: 2, 13: 2, 14: 2,\n",
        "                                 15: 3, 16: 3, 17: 3, 18: 3, 19: 3,\n",
        "                                 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4})\n",
        "\n",
        "'''\n",
        "# Change values in job column from Sinh vien trung cap to Trung cap nghe\n",
        "data.Job = data.Job.replace({'Học sinh cấp 3': 0,\n",
        "                             'Sinh viên trung cấp': 1,\n",
        "                             'Sinh viên năm 1': 2,\n",
        "                             'Sinh viên năm 2': 3,\n",
        "                             'Sinh viên năm 3': 4,\n",
        "                             'Sinh viên năm 4': 5,\n",
        "                             'Sinh viên từ năm 4 trở lên': 6,\n",
        "                             'Đã tốt nghiệp': 7}) '''\n",
        "# LGBT\n",
        "data.LGBT = data.LGBT.replace({'Có': 1,\n",
        "                               'Không': 0,\n",
        "                               'Chưa xác định': 0.5})\n",
        "'''\n",
        "# Sex\n",
        "data.Sex = data.Sex.replace({'Nam': 1,'Nữ': 0})\n",
        "\n",
        "# Per - Intro (A)\n",
        "data.a1 = data.a1.replace({'Hướng ngoại': 1,\n",
        "                     'Hướng nội': 2,\n",
        "                     'Chưa xác định được': 0,\n",
        "                     'Vừa hướng nội vừa hướng ngoại': 1.5}) '''\n",
        "data.a2 = data.a2.replace({'Có': 1, 'Một chút': 0.5, 'Không': 0})\n",
        "data.a3 = data.a3.replace({'Có': 1, 'Một chút': 0.5, 'Không': 0})\n",
        "data.a4 = data.a4.replace({'Có': 1, 'Một chút': 0.5, 'Không': 0})\n",
        "#data.a5 = data.a5.replace({'Có': 1, 'Một chút': 0.5, 'Không': 0})\n",
        "#data.a6 = data.a6.replace({'Có': 1, 'Một chút': 0.5, 'Không': 0})\n",
        "\n",
        "# Question (B)\n",
        "#data.b6 = data.b1.replace({'Phần lớn dành cho việc vui chơi': 0, 'Cân đối giữa học tập và vui chơi': 0.5, 'Phần lớn dành cho học tập': 1})\n",
        "data.b15 = data.b15.replace({'Chưa từng': 0, 'Một - hai lần': 1, 'Khoảng năm lần': 2, 'Khoảng 10 lần': 3, 'Rất nhiều': 4, 'Mỗi ngày': 5})\n",
        "\n",
        "# Fill null value with the most popular value.\n",
        "data = data.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
        "\n",
        "# Drop the column Name\n",
        "data = data.drop(columns=[ 'Name', 'DoB', 'a7', 'a8', 'Job', 'Sex', 'a5', 'a6', 'b5', 'b14','b7', 'b2', 'b9', 'b4', 'b1', 'b3', 'b6', 'a1','b10', 'b12', 'a2', 'a4', 'b15'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "qV8WW_fuzzGA",
        "outputId": "f3a4be14-b01e-43ae-cbe8-6b2904ec7b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   LGBT   a3   b8  b11  b13   c1   c2   c3   c4   c5   c6   c7   c8   c9  \\\n",
              "0   1.0  0.5  4.0  3.0  4.0  2.0  1.0  3.0  0.0  2.0  1.0  1.0  2.0  1.0   \n",
              "1   0.0  0.0  2.0  1.0  2.0  3.0  2.0  2.0  1.0  1.0  0.0  1.0  1.0  0.0   \n",
              "2   0.0  0.0  3.0  2.0  3.0  3.0  3.0  1.0  3.0  3.0  1.0  2.0  1.0  0.0   \n",
              "3   0.0  0.0  3.0  1.0  2.0  2.0  1.0  1.0  3.0  3.0  1.0  1.0  1.0  0.0   \n",
              "4   0.0  0.5  5.0  3.0  2.0  1.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0   \n",
              "\n",
              "   Total  \n",
              "0    2.0  \n",
              "1    2.0  \n",
              "2    3.0  \n",
              "3    2.0  \n",
              "4    0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-65c9e68d-3f4b-4bd0-afeb-4800ff12984e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LGBT</th>\n",
              "      <th>a3</th>\n",
              "      <th>b8</th>\n",
              "      <th>b11</th>\n",
              "      <th>b13</th>\n",
              "      <th>c1</th>\n",
              "      <th>c2</th>\n",
              "      <th>c3</th>\n",
              "      <th>c4</th>\n",
              "      <th>c5</th>\n",
              "      <th>c6</th>\n",
              "      <th>c7</th>\n",
              "      <th>c8</th>\n",
              "      <th>c9</th>\n",
              "      <th>Total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-65c9e68d-3f4b-4bd0-afeb-4800ff12984e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-65c9e68d-3f4b-4bd0-afeb-4800ff12984e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-65c9e68d-3f4b-4bd0-afeb-4800ff12984e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxVt0DFE4pdg",
        "outputId": "8667f357-c613-4163-8e78-3e37e6f5359e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 291 entries, 0 to 290\n",
            "Data columns (total 15 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   LGBT    291 non-null    float64\n",
            " 1   a3      291 non-null    float64\n",
            " 2   b8      291 non-null    float64\n",
            " 3   b11     291 non-null    float64\n",
            " 4   b13     291 non-null    float64\n",
            " 5   c1      291 non-null    float64\n",
            " 6   c2      291 non-null    float64\n",
            " 7   c3      291 non-null    float64\n",
            " 8   c4      291 non-null    float64\n",
            " 9   c5      291 non-null    float64\n",
            " 10  c6      291 non-null    float64\n",
            " 11  c7      291 non-null    float64\n",
            " 12  c8      291 non-null    float64\n",
            " 13  c9      291 non-null    float64\n",
            " 14  Total   291 non-null    float64\n",
            "dtypes: float64(15)\n",
            "memory usage: 34.2 KB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg7pJ2lL4wnI"
      },
      "source": [
        "# Step 2: Traing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "BJGE9c694sR9",
        "outputId": "cb30095f-c537-4cbf-9bbf-0b46c9dde0a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: this method is deprecated in favour of `Styler.format(precision=..)`\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7fc11981cd50>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_800eb_row0_col0, #T_800eb_row1_col1, #T_800eb_row2_col2, #T_800eb_row3_col3, #T_800eb_row4_col4, #T_800eb_row5_col5, #T_800eb_row6_col6, #T_800eb_row7_col7, #T_800eb_row8_col8, #T_800eb_row9_col9, #T_800eb_row10_col10, #T_800eb_row11_col11, #T_800eb_row12_col12, #T_800eb_row13_col13, #T_800eb_row14_col14 {\n",
              "  background-color: #b40426;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row0_col1, #T_800eb_row0_col2, #T_800eb_row0_col3, #T_800eb_row0_col4, #T_800eb_row0_col5, #T_800eb_row0_col6, #T_800eb_row0_col7, #T_800eb_row0_col8, #T_800eb_row0_col9, #T_800eb_row0_col10, #T_800eb_row0_col11, #T_800eb_row0_col12, #T_800eb_row0_col13, #T_800eb_row0_col14, #T_800eb_row12_col0 {\n",
              "  background-color: #3b4cc0;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row1_col0 {\n",
              "  background-color: #3c4ec2;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row1_col2, #T_800eb_row6_col8, #T_800eb_row9_col1 {\n",
              "  background-color: #9ebeff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row1_col3 {\n",
              "  background-color: #7396f5;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row1_col4, #T_800eb_row3_col1, #T_800eb_row8_col3 {\n",
              "  background-color: #7699f6;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row1_col5, #T_800eb_row2_col5, #T_800eb_row5_col3, #T_800eb_row5_col8, #T_800eb_row6_col4, #T_800eb_row6_col13, #T_800eb_row12_col8 {\n",
              "  background-color: #90b2fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row1_col6, #T_800eb_row7_col9, #T_800eb_row13_col4 {\n",
              "  background-color: #abc8fd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row1_col7 {\n",
              "  background-color: #799cf8;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row1_col8, #T_800eb_row2_col8 {\n",
              "  background-color: #7a9df8;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row1_col9, #T_800eb_row1_col13, #T_800eb_row13_col9 {\n",
              "  background-color: #8db0fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row1_col10, #T_800eb_row5_col7, #T_800eb_row10_col6 {\n",
              "  background-color: #c4d5f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row1_col11, #T_800eb_row1_col12, #T_800eb_row6_col2, #T_800eb_row8_col6, #T_800eb_row11_col4 {\n",
              "  background-color: #b5cdfa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row1_col14 {\n",
              "  background-color: #c3d5f4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row2_col0, #T_800eb_row9_col0 {\n",
              "  background-color: #4f69d9;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row2_col1, #T_800eb_row10_col8 {\n",
              "  background-color: #aec9fc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row2_col3, #T_800eb_row4_col5, #T_800eb_row4_col7, #T_800eb_row7_col4, #T_800eb_row9_col7, #T_800eb_row10_col4 {\n",
              "  background-color: #a3c2fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row2_col4 {\n",
              "  background-color: #a7c5fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row2_col6, #T_800eb_row11_col7 {\n",
              "  background-color: #c1d4f4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row2_col7, #T_800eb_row4_col1, #T_800eb_row4_col12, #T_800eb_row8_col5, #T_800eb_row9_col5 {\n",
              "  background-color: #93b5fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row2_col9, #T_800eb_row3_col2, #T_800eb_row5_col2, #T_800eb_row8_col1, #T_800eb_row9_col3 {\n",
              "  background-color: #96b7ff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row2_col10, #T_800eb_row7_col11, #T_800eb_row10_col9, #T_800eb_row14_col2 {\n",
              "  background-color: #d2dbe8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row2_col11, #T_800eb_row4_col11, #T_800eb_row6_col7 {\n",
              "  background-color: #c7d7f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row2_col12, #T_800eb_row4_col3, #T_800eb_row8_col9, #T_800eb_row11_col5, #T_800eb_row13_col12 {\n",
              "  background-color: #bfd3f6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row2_col13 {\n",
              "  background-color: #779af7;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row2_col14, #T_800eb_row6_col5 {\n",
              "  background-color: #c9d7f0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row3_col0, #T_800eb_row11_col0 {\n",
              "  background-color: #3f53c6;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row3_col4, #T_800eb_row4_col13, #T_800eb_row8_col12, #T_800eb_row12_col7, #T_800eb_row13_col5, #T_800eb_row13_col6 {\n",
              "  background-color: #aac7fd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row3_col5 {\n",
              "  background-color: #7b9ff9;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row3_col6, #T_800eb_row6_col3, #T_800eb_row7_col3 {\n",
              "  background-color: #a1c0ff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row3_col7, #T_800eb_row13_col7 {\n",
              "  background-color: #89acfd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row3_col8 {\n",
              "  background-color: #5b7ae5;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row3_col9 {\n",
              "  background-color: #88abfd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row3_col10, #T_800eb_row3_col11, #T_800eb_row3_col14, #T_800eb_row9_col8, #T_800eb_row11_col1 {\n",
              "  background-color: #b7cff9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row3_col12 {\n",
              "  background-color: #97b8ff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row3_col13 {\n",
              "  background-color: #98b9ff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row4_col0, #T_800eb_row7_col0, #T_800eb_row8_col0 {\n",
              "  background-color: #5a78e4;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row4_col2, #T_800eb_row10_col5, #T_800eb_row12_col2 {\n",
              "  background-color: #b1cbfc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row4_col6, #T_800eb_row13_col1 {\n",
              "  background-color: #a9c6fd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row4_col8, #T_800eb_row9_col13 {\n",
              "  background-color: #81a4fb;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row4_col9, #T_800eb_row7_col2 {\n",
              "  background-color: #9dbdff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row4_col10, #T_800eb_row12_col1 {\n",
              "  background-color: #b3cdfb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row4_col14 {\n",
              "  background-color: #cedaeb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row5_col0 {\n",
              "  background-color: #5673e0;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row5_col1, #T_800eb_row5_col12 {\n",
              "  background-color: #a6c4fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row5_col4 {\n",
              "  background-color: #9fbfff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row5_col6 {\n",
              "  background-color: #d5dbe5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row5_col9 {\n",
              "  background-color: #9abbff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row5_col10, #T_800eb_row8_col10 {\n",
              "  background-color: #bed2f6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row5_col11, #T_800eb_row10_col2, #T_800eb_row10_col13, #T_800eb_row11_col9, #T_800eb_row14_col3 {\n",
              "  background-color: #ccd9ed;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row5_col13, #T_800eb_row12_col13 {\n",
              "  background-color: #a5c3fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row5_col14 {\n",
              "  background-color: #f4c6af;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row6_col0 {\n",
              "  background-color: #3e51c5;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row6_col1 {\n",
              "  background-color: #adc9fd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row6_col9, #T_800eb_row10_col3 {\n",
              "  background-color: #bbd1f8;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row6_col10, #T_800eb_row7_col10, #T_800eb_row7_col12 {\n",
              "  background-color: #c0d4f5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row6_col11, #T_800eb_row11_col6 {\n",
              "  background-color: #e6d7cf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row6_col12 {\n",
              "  background-color: #dddcdc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row6_col14 {\n",
              "  background-color: #f6bda2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row7_col1, #T_800eb_row7_col8, #T_800eb_row8_col7, #T_800eb_row9_col2 {\n",
              "  background-color: #94b6ff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row7_col5, #T_800eb_row9_col6 {\n",
              "  background-color: #c6d6f1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row7_col6 {\n",
              "  background-color: #d6dce4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row7_col13 {\n",
              "  background-color: #86a9fc;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row7_col14 {\n",
              "  background-color: #f5c0a7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row8_col2, #T_800eb_row13_col2 {\n",
              "  background-color: #84a7fc;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row8_col4, #T_800eb_row8_col13 {\n",
              "  background-color: #80a3fa;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row8_col11, #T_800eb_row13_col10 {\n",
              "  background-color: #d8dce2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row8_col14 {\n",
              "  background-color: #edd2c3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row9_col4, #T_800eb_row12_col3 {\n",
              "  background-color: #92b4fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row9_col10, #T_800eb_row14_col1 {\n",
              "  background-color: #d7dce3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row9_col11, #T_800eb_row11_col13 {\n",
              "  background-color: #d4dbe6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row9_col12, #T_800eb_row12_col6 {\n",
              "  background-color: #dcdddd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row9_col14 {\n",
              "  background-color: #f2cab5;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row10_col0 {\n",
              "  background-color: #455cce;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row10_col1, #T_800eb_row11_col8 {\n",
              "  background-color: #cad8ef;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row10_col7 {\n",
              "  background-color: #b2ccfb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row10_col11 {\n",
              "  background-color: #f1cdba;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row10_col12 {\n",
              "  background-color: #dedcdb;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row10_col14 {\n",
              "  background-color: #f7b093;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row11_col2 {\n",
              "  background-color: #bcd2f7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row11_col3 {\n",
              "  background-color: #b6cefa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row11_col10 {\n",
              "  background-color: #efcebd;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row11_col12 {\n",
              "  background-color: #f3c8b2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row11_col14 {\n",
              "  background-color: #f39778;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row12_col4 {\n",
              "  background-color: #7597f6;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row12_col5 {\n",
              "  background-color: #8fb1fe;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row12_col9 {\n",
              "  background-color: #d3dbe7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row12_col10 {\n",
              "  background-color: #d9dce1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row12_col11 {\n",
              "  background-color: #f2c9b4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row12_col14 {\n",
              "  background-color: #f7b497;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row13_col0 {\n",
              "  background-color: #5d7ce6;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row13_col3 {\n",
              "  background-color: #afcafc;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row13_col8 {\n",
              "  background-color: #82a6fb;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row13_col11 {\n",
              "  background-color: #e1dad6;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row13_col14, #T_800eb_row14_col8 {\n",
              "  background-color: #eed0c0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row14_col0 {\n",
              "  background-color: #5f7fe8;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row14_col4 {\n",
              "  background-color: #d1dae9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row14_col5 {\n",
              "  background-color: #f5c2aa;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row14_col6 {\n",
              "  background-color: #f7b194;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row14_col7 {\n",
              "  background-color: #f6bea4;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row14_col9 {\n",
              "  background-color: #f5c4ac;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row14_col10 {\n",
              "  background-color: #f7a688;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row14_col11 {\n",
              "  background-color: #f08b6e;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_800eb_row14_col12 {\n",
              "  background-color: #f7a889;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_800eb_row14_col13 {\n",
              "  background-color: #efcfbf;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_800eb_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >LGBT</th>\n",
              "      <th class=\"col_heading level0 col1\" >a3</th>\n",
              "      <th class=\"col_heading level0 col2\" >b8</th>\n",
              "      <th class=\"col_heading level0 col3\" >b11</th>\n",
              "      <th class=\"col_heading level0 col4\" >b13</th>\n",
              "      <th class=\"col_heading level0 col5\" >c1</th>\n",
              "      <th class=\"col_heading level0 col6\" >c2</th>\n",
              "      <th class=\"col_heading level0 col7\" >c3</th>\n",
              "      <th class=\"col_heading level0 col8\" >c4</th>\n",
              "      <th class=\"col_heading level0 col9\" >c5</th>\n",
              "      <th class=\"col_heading level0 col10\" >c6</th>\n",
              "      <th class=\"col_heading level0 col11\" >c7</th>\n",
              "      <th class=\"col_heading level0 col12\" >c8</th>\n",
              "      <th class=\"col_heading level0 col13\" >c9</th>\n",
              "      <th class=\"col_heading level0 col14\" >Total</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row0\" class=\"row_heading level0 row0\" >LGBT</th>\n",
              "      <td id=\"T_800eb_row0_col0\" class=\"data row0 col0\" >1.00</td>\n",
              "      <td id=\"T_800eb_row0_col1\" class=\"data row0 col1\" >0.07</td>\n",
              "      <td id=\"T_800eb_row0_col2\" class=\"data row0 col2\" >0.13</td>\n",
              "      <td id=\"T_800eb_row0_col3\" class=\"data row0 col3\" >0.09</td>\n",
              "      <td id=\"T_800eb_row0_col4\" class=\"data row0 col4\" >0.17</td>\n",
              "      <td id=\"T_800eb_row0_col5\" class=\"data row0 col5\" >0.15</td>\n",
              "      <td id=\"T_800eb_row0_col6\" class=\"data row0 col6\" >0.08</td>\n",
              "      <td id=\"T_800eb_row0_col7\" class=\"data row0 col7\" >0.16</td>\n",
              "      <td id=\"T_800eb_row0_col8\" class=\"data row0 col8\" >0.16</td>\n",
              "      <td id=\"T_800eb_row0_col9\" class=\"data row0 col9\" >0.13</td>\n",
              "      <td id=\"T_800eb_row0_col10\" class=\"data row0 col10\" >0.10</td>\n",
              "      <td id=\"T_800eb_row0_col11\" class=\"data row0 col11\" >0.09</td>\n",
              "      <td id=\"T_800eb_row0_col12\" class=\"data row0 col12\" >0.07</td>\n",
              "      <td id=\"T_800eb_row0_col13\" class=\"data row0 col13\" >0.17</td>\n",
              "      <td id=\"T_800eb_row0_col14\" class=\"data row0 col14\" >0.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row1\" class=\"row_heading level0 row1\" >a3</th>\n",
              "      <td id=\"T_800eb_row1_col0\" class=\"data row1 col0\" >0.07</td>\n",
              "      <td id=\"T_800eb_row1_col1\" class=\"data row1 col1\" >1.00</td>\n",
              "      <td id=\"T_800eb_row1_col2\" class=\"data row1 col2\" >0.39</td>\n",
              "      <td id=\"T_800eb_row1_col3\" class=\"data row1 col3\" >0.25</td>\n",
              "      <td id=\"T_800eb_row1_col4\" class=\"data row1 col4\" >0.32</td>\n",
              "      <td id=\"T_800eb_row1_col5\" class=\"data row1 col5\" >0.37</td>\n",
              "      <td id=\"T_800eb_row1_col6\" class=\"data row1 col6\" >0.39</td>\n",
              "      <td id=\"T_800eb_row1_col7\" class=\"data row1 col7\" >0.33</td>\n",
              "      <td id=\"T_800eb_row1_col8\" class=\"data row1 col8\" >0.33</td>\n",
              "      <td id=\"T_800eb_row1_col9\" class=\"data row1 col9\" >0.35</td>\n",
              "      <td id=\"T_800eb_row1_col10\" class=\"data row1 col10\" >0.47</td>\n",
              "      <td id=\"T_800eb_row1_col11\" class=\"data row1 col11\" >0.42</td>\n",
              "      <td id=\"T_800eb_row1_col12\" class=\"data row1 col12\" >0.41</td>\n",
              "      <td id=\"T_800eb_row1_col13\" class=\"data row1 col13\" >0.38</td>\n",
              "      <td id=\"T_800eb_row1_col14\" class=\"data row1 col14\" >0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row2\" class=\"row_heading level0 row2\" >b8</th>\n",
              "      <td id=\"T_800eb_row2_col0\" class=\"data row2 col0\" >0.13</td>\n",
              "      <td id=\"T_800eb_row2_col1\" class=\"data row2 col1\" >0.39</td>\n",
              "      <td id=\"T_800eb_row2_col2\" class=\"data row2 col2\" >1.00</td>\n",
              "      <td id=\"T_800eb_row2_col3\" class=\"data row2 col3\" >0.37</td>\n",
              "      <td id=\"T_800eb_row2_col4\" class=\"data row2 col4\" >0.44</td>\n",
              "      <td id=\"T_800eb_row2_col5\" class=\"data row2 col5\" >0.37</td>\n",
              "      <td id=\"T_800eb_row2_col6\" class=\"data row2 col6\" >0.45</td>\n",
              "      <td id=\"T_800eb_row2_col7\" class=\"data row2 col7\" >0.39</td>\n",
              "      <td id=\"T_800eb_row2_col8\" class=\"data row2 col8\" >0.33</td>\n",
              "      <td id=\"T_800eb_row2_col9\" class=\"data row2 col9\" >0.37</td>\n",
              "      <td id=\"T_800eb_row2_col10\" class=\"data row2 col10\" >0.52</td>\n",
              "      <td id=\"T_800eb_row2_col11\" class=\"data row2 col11\" >0.47</td>\n",
              "      <td id=\"T_800eb_row2_col12\" class=\"data row2 col12\" >0.44</td>\n",
              "      <td id=\"T_800eb_row2_col13\" class=\"data row2 col13\" >0.33</td>\n",
              "      <td id=\"T_800eb_row2_col14\" class=\"data row2 col14\" >0.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row3\" class=\"row_heading level0 row3\" >b11</th>\n",
              "      <td id=\"T_800eb_row3_col0\" class=\"data row3 col0\" >0.09</td>\n",
              "      <td id=\"T_800eb_row3_col1\" class=\"data row3 col1\" >0.25</td>\n",
              "      <td id=\"T_800eb_row3_col2\" class=\"data row3 col2\" >0.37</td>\n",
              "      <td id=\"T_800eb_row3_col3\" class=\"data row3 col3\" >1.00</td>\n",
              "      <td id=\"T_800eb_row3_col4\" class=\"data row3 col4\" >0.45</td>\n",
              "      <td id=\"T_800eb_row3_col5\" class=\"data row3 col5\" >0.32</td>\n",
              "      <td id=\"T_800eb_row3_col6\" class=\"data row3 col6\" >0.36</td>\n",
              "      <td id=\"T_800eb_row3_col7\" class=\"data row3 col7\" >0.37</td>\n",
              "      <td id=\"T_800eb_row3_col8\" class=\"data row3 col8\" >0.25</td>\n",
              "      <td id=\"T_800eb_row3_col9\" class=\"data row3 col9\" >0.34</td>\n",
              "      <td id=\"T_800eb_row3_col10\" class=\"data row3 col10\" >0.44</td>\n",
              "      <td id=\"T_800eb_row3_col11\" class=\"data row3 col11\" >0.42</td>\n",
              "      <td id=\"T_800eb_row3_col12\" class=\"data row3 col12\" >0.33</td>\n",
              "      <td id=\"T_800eb_row3_col13\" class=\"data row3 col13\" >0.41</td>\n",
              "      <td id=\"T_800eb_row3_col14\" class=\"data row3 col14\" >0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row4\" class=\"row_heading level0 row4\" >b13</th>\n",
              "      <td id=\"T_800eb_row4_col0\" class=\"data row4 col0\" >0.17</td>\n",
              "      <td id=\"T_800eb_row4_col1\" class=\"data row4 col1\" >0.32</td>\n",
              "      <td id=\"T_800eb_row4_col2\" class=\"data row4 col2\" >0.44</td>\n",
              "      <td id=\"T_800eb_row4_col3\" class=\"data row4 col3\" >0.45</td>\n",
              "      <td id=\"T_800eb_row4_col4\" class=\"data row4 col4\" >1.00</td>\n",
              "      <td id=\"T_800eb_row4_col5\" class=\"data row4 col5\" >0.42</td>\n",
              "      <td id=\"T_800eb_row4_col6\" class=\"data row4 col6\" >0.38</td>\n",
              "      <td id=\"T_800eb_row4_col7\" class=\"data row4 col7\" >0.43</td>\n",
              "      <td id=\"T_800eb_row4_col8\" class=\"data row4 col8\" >0.35</td>\n",
              "      <td id=\"T_800eb_row4_col9\" class=\"data row4 col9\" >0.39</td>\n",
              "      <td id=\"T_800eb_row4_col10\" class=\"data row4 col10\" >0.43</td>\n",
              "      <td id=\"T_800eb_row4_col11\" class=\"data row4 col11\" >0.47</td>\n",
              "      <td id=\"T_800eb_row4_col12\" class=\"data row4 col12\" >0.32</td>\n",
              "      <td id=\"T_800eb_row4_col13\" class=\"data row4 col13\" >0.45</td>\n",
              "      <td id=\"T_800eb_row4_col14\" class=\"data row4 col14\" >0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row5\" class=\"row_heading level0 row5\" >c1</th>\n",
              "      <td id=\"T_800eb_row5_col0\" class=\"data row5 col0\" >0.15</td>\n",
              "      <td id=\"T_800eb_row5_col1\" class=\"data row5 col1\" >0.37</td>\n",
              "      <td id=\"T_800eb_row5_col2\" class=\"data row5 col2\" >0.37</td>\n",
              "      <td id=\"T_800eb_row5_col3\" class=\"data row5 col3\" >0.32</td>\n",
              "      <td id=\"T_800eb_row5_col4\" class=\"data row5 col4\" >0.42</td>\n",
              "      <td id=\"T_800eb_row5_col5\" class=\"data row5 col5\" >1.00</td>\n",
              "      <td id=\"T_800eb_row5_col6\" class=\"data row5 col6\" >0.51</td>\n",
              "      <td id=\"T_800eb_row5_col7\" class=\"data row5 col7\" >0.51</td>\n",
              "      <td id=\"T_800eb_row5_col8\" class=\"data row5 col8\" >0.38</td>\n",
              "      <td id=\"T_800eb_row5_col9\" class=\"data row5 col9\" >0.38</td>\n",
              "      <td id=\"T_800eb_row5_col10\" class=\"data row5 col10\" >0.45</td>\n",
              "      <td id=\"T_800eb_row5_col11\" class=\"data row5 col11\" >0.49</td>\n",
              "      <td id=\"T_800eb_row5_col12\" class=\"data row5 col12\" >0.37</td>\n",
              "      <td id=\"T_800eb_row5_col13\" class=\"data row5 col13\" >0.44</td>\n",
              "      <td id=\"T_800eb_row5_col14\" class=\"data row5 col14\" >0.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row6\" class=\"row_heading level0 row6\" >c2</th>\n",
              "      <td id=\"T_800eb_row6_col0\" class=\"data row6 col0\" >0.08</td>\n",
              "      <td id=\"T_800eb_row6_col1\" class=\"data row6 col1\" >0.39</td>\n",
              "      <td id=\"T_800eb_row6_col2\" class=\"data row6 col2\" >0.45</td>\n",
              "      <td id=\"T_800eb_row6_col3\" class=\"data row6 col3\" >0.36</td>\n",
              "      <td id=\"T_800eb_row6_col4\" class=\"data row6 col4\" >0.38</td>\n",
              "      <td id=\"T_800eb_row6_col5\" class=\"data row6 col5\" >0.51</td>\n",
              "      <td id=\"T_800eb_row6_col6\" class=\"data row6 col6\" >1.00</td>\n",
              "      <td id=\"T_800eb_row6_col7\" class=\"data row6 col7\" >0.52</td>\n",
              "      <td id=\"T_800eb_row6_col8\" class=\"data row6 col8\" >0.42</td>\n",
              "      <td id=\"T_800eb_row6_col9\" class=\"data row6 col9\" >0.47</td>\n",
              "      <td id=\"T_800eb_row6_col10\" class=\"data row6 col10\" >0.46</td>\n",
              "      <td id=\"T_800eb_row6_col11\" class=\"data row6 col11\" >0.58</td>\n",
              "      <td id=\"T_800eb_row6_col12\" class=\"data row6 col12\" >0.54</td>\n",
              "      <td id=\"T_800eb_row6_col13\" class=\"data row6 col13\" >0.39</td>\n",
              "      <td id=\"T_800eb_row6_col14\" class=\"data row6 col14\" >0.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row7\" class=\"row_heading level0 row7\" >c3</th>\n",
              "      <td id=\"T_800eb_row7_col0\" class=\"data row7 col0\" >0.16</td>\n",
              "      <td id=\"T_800eb_row7_col1\" class=\"data row7 col1\" >0.33</td>\n",
              "      <td id=\"T_800eb_row7_col2\" class=\"data row7 col2\" >0.39</td>\n",
              "      <td id=\"T_800eb_row7_col3\" class=\"data row7 col3\" >0.37</td>\n",
              "      <td id=\"T_800eb_row7_col4\" class=\"data row7 col4\" >0.43</td>\n",
              "      <td id=\"T_800eb_row7_col5\" class=\"data row7 col5\" >0.51</td>\n",
              "      <td id=\"T_800eb_row7_col6\" class=\"data row7 col6\" >0.52</td>\n",
              "      <td id=\"T_800eb_row7_col7\" class=\"data row7 col7\" >1.00</td>\n",
              "      <td id=\"T_800eb_row7_col8\" class=\"data row7 col8\" >0.39</td>\n",
              "      <td id=\"T_800eb_row7_col9\" class=\"data row7 col9\" >0.43</td>\n",
              "      <td id=\"T_800eb_row7_col10\" class=\"data row7 col10\" >0.46</td>\n",
              "      <td id=\"T_800eb_row7_col11\" class=\"data row7 col11\" >0.50</td>\n",
              "      <td id=\"T_800eb_row7_col12\" class=\"data row7 col12\" >0.44</td>\n",
              "      <td id=\"T_800eb_row7_col13\" class=\"data row7 col13\" >0.36</td>\n",
              "      <td id=\"T_800eb_row7_col14\" class=\"data row7 col14\" >0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row8\" class=\"row_heading level0 row8\" >c4</th>\n",
              "      <td id=\"T_800eb_row8_col0\" class=\"data row8 col0\" >0.16</td>\n",
              "      <td id=\"T_800eb_row8_col1\" class=\"data row8 col1\" >0.33</td>\n",
              "      <td id=\"T_800eb_row8_col2\" class=\"data row8 col2\" >0.33</td>\n",
              "      <td id=\"T_800eb_row8_col3\" class=\"data row8 col3\" >0.25</td>\n",
              "      <td id=\"T_800eb_row8_col4\" class=\"data row8 col4\" >0.35</td>\n",
              "      <td id=\"T_800eb_row8_col5\" class=\"data row8 col5\" >0.38</td>\n",
              "      <td id=\"T_800eb_row8_col6\" class=\"data row8 col6\" >0.42</td>\n",
              "      <td id=\"T_800eb_row8_col7\" class=\"data row8 col7\" >0.39</td>\n",
              "      <td id=\"T_800eb_row8_col8\" class=\"data row8 col8\" >1.00</td>\n",
              "      <td id=\"T_800eb_row8_col9\" class=\"data row8 col9\" >0.48</td>\n",
              "      <td id=\"T_800eb_row8_col10\" class=\"data row8 col10\" >0.45</td>\n",
              "      <td id=\"T_800eb_row8_col11\" class=\"data row8 col11\" >0.53</td>\n",
              "      <td id=\"T_800eb_row8_col12\" class=\"data row8 col12\" >0.38</td>\n",
              "      <td id=\"T_800eb_row8_col13\" class=\"data row8 col13\" >0.35</td>\n",
              "      <td id=\"T_800eb_row8_col14\" class=\"data row8 col14\" >0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row9\" class=\"row_heading level0 row9\" >c5</th>\n",
              "      <td id=\"T_800eb_row9_col0\" class=\"data row9 col0\" >0.13</td>\n",
              "      <td id=\"T_800eb_row9_col1\" class=\"data row9 col1\" >0.35</td>\n",
              "      <td id=\"T_800eb_row9_col2\" class=\"data row9 col2\" >0.37</td>\n",
              "      <td id=\"T_800eb_row9_col3\" class=\"data row9 col3\" >0.34</td>\n",
              "      <td id=\"T_800eb_row9_col4\" class=\"data row9 col4\" >0.39</td>\n",
              "      <td id=\"T_800eb_row9_col5\" class=\"data row9 col5\" >0.38</td>\n",
              "      <td id=\"T_800eb_row9_col6\" class=\"data row9 col6\" >0.47</td>\n",
              "      <td id=\"T_800eb_row9_col7\" class=\"data row9 col7\" >0.43</td>\n",
              "      <td id=\"T_800eb_row9_col8\" class=\"data row9 col8\" >0.48</td>\n",
              "      <td id=\"T_800eb_row9_col9\" class=\"data row9 col9\" >1.00</td>\n",
              "      <td id=\"T_800eb_row9_col10\" class=\"data row9 col10\" >0.53</td>\n",
              "      <td id=\"T_800eb_row9_col11\" class=\"data row9 col11\" >0.51</td>\n",
              "      <td id=\"T_800eb_row9_col12\" class=\"data row9 col12\" >0.53</td>\n",
              "      <td id=\"T_800eb_row9_col13\" class=\"data row9 col13\" >0.35</td>\n",
              "      <td id=\"T_800eb_row9_col14\" class=\"data row9 col14\" >0.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row10\" class=\"row_heading level0 row10\" >c6</th>\n",
              "      <td id=\"T_800eb_row10_col0\" class=\"data row10 col0\" >0.10</td>\n",
              "      <td id=\"T_800eb_row10_col1\" class=\"data row10 col1\" >0.47</td>\n",
              "      <td id=\"T_800eb_row10_col2\" class=\"data row10 col2\" >0.52</td>\n",
              "      <td id=\"T_800eb_row10_col3\" class=\"data row10 col3\" >0.44</td>\n",
              "      <td id=\"T_800eb_row10_col4\" class=\"data row10 col4\" >0.43</td>\n",
              "      <td id=\"T_800eb_row10_col5\" class=\"data row10 col5\" >0.45</td>\n",
              "      <td id=\"T_800eb_row10_col6\" class=\"data row10 col6\" >0.46</td>\n",
              "      <td id=\"T_800eb_row10_col7\" class=\"data row10 col7\" >0.46</td>\n",
              "      <td id=\"T_800eb_row10_col8\" class=\"data row10 col8\" >0.45</td>\n",
              "      <td id=\"T_800eb_row10_col9\" class=\"data row10 col9\" >0.53</td>\n",
              "      <td id=\"T_800eb_row10_col10\" class=\"data row10 col10\" >1.00</td>\n",
              "      <td id=\"T_800eb_row10_col11\" class=\"data row10 col11\" >0.63</td>\n",
              "      <td id=\"T_800eb_row10_col12\" class=\"data row10 col12\" >0.54</td>\n",
              "      <td id=\"T_800eb_row10_col13\" class=\"data row10 col13\" >0.54</td>\n",
              "      <td id=\"T_800eb_row10_col14\" class=\"data row10 col14\" >0.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row11\" class=\"row_heading level0 row11\" >c7</th>\n",
              "      <td id=\"T_800eb_row11_col0\" class=\"data row11 col0\" >0.09</td>\n",
              "      <td id=\"T_800eb_row11_col1\" class=\"data row11 col1\" >0.42</td>\n",
              "      <td id=\"T_800eb_row11_col2\" class=\"data row11 col2\" >0.47</td>\n",
              "      <td id=\"T_800eb_row11_col3\" class=\"data row11 col3\" >0.42</td>\n",
              "      <td id=\"T_800eb_row11_col4\" class=\"data row11 col4\" >0.47</td>\n",
              "      <td id=\"T_800eb_row11_col5\" class=\"data row11 col5\" >0.49</td>\n",
              "      <td id=\"T_800eb_row11_col6\" class=\"data row11 col6\" >0.58</td>\n",
              "      <td id=\"T_800eb_row11_col7\" class=\"data row11 col7\" >0.50</td>\n",
              "      <td id=\"T_800eb_row11_col8\" class=\"data row11 col8\" >0.53</td>\n",
              "      <td id=\"T_800eb_row11_col9\" class=\"data row11 col9\" >0.51</td>\n",
              "      <td id=\"T_800eb_row11_col10\" class=\"data row11 col10\" >0.63</td>\n",
              "      <td id=\"T_800eb_row11_col11\" class=\"data row11 col11\" >1.00</td>\n",
              "      <td id=\"T_800eb_row11_col12\" class=\"data row11 col12\" >0.64</td>\n",
              "      <td id=\"T_800eb_row11_col13\" class=\"data row11 col13\" >0.56</td>\n",
              "      <td id=\"T_800eb_row11_col14\" class=\"data row11 col14\" >0.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row12\" class=\"row_heading level0 row12\" >c8</th>\n",
              "      <td id=\"T_800eb_row12_col0\" class=\"data row12 col0\" >0.07</td>\n",
              "      <td id=\"T_800eb_row12_col1\" class=\"data row12 col1\" >0.41</td>\n",
              "      <td id=\"T_800eb_row12_col2\" class=\"data row12 col2\" >0.44</td>\n",
              "      <td id=\"T_800eb_row12_col3\" class=\"data row12 col3\" >0.33</td>\n",
              "      <td id=\"T_800eb_row12_col4\" class=\"data row12 col4\" >0.32</td>\n",
              "      <td id=\"T_800eb_row12_col5\" class=\"data row12 col5\" >0.37</td>\n",
              "      <td id=\"T_800eb_row12_col6\" class=\"data row12 col6\" >0.54</td>\n",
              "      <td id=\"T_800eb_row12_col7\" class=\"data row12 col7\" >0.44</td>\n",
              "      <td id=\"T_800eb_row12_col8\" class=\"data row12 col8\" >0.38</td>\n",
              "      <td id=\"T_800eb_row12_col9\" class=\"data row12 col9\" >0.53</td>\n",
              "      <td id=\"T_800eb_row12_col10\" class=\"data row12 col10\" >0.54</td>\n",
              "      <td id=\"T_800eb_row12_col11\" class=\"data row12 col11\" >0.64</td>\n",
              "      <td id=\"T_800eb_row12_col12\" class=\"data row12 col12\" >1.00</td>\n",
              "      <td id=\"T_800eb_row12_col13\" class=\"data row12 col13\" >0.44</td>\n",
              "      <td id=\"T_800eb_row12_col14\" class=\"data row12 col14\" >0.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row13\" class=\"row_heading level0 row13\" >c9</th>\n",
              "      <td id=\"T_800eb_row13_col0\" class=\"data row13 col0\" >0.17</td>\n",
              "      <td id=\"T_800eb_row13_col1\" class=\"data row13 col1\" >0.38</td>\n",
              "      <td id=\"T_800eb_row13_col2\" class=\"data row13 col2\" >0.33</td>\n",
              "      <td id=\"T_800eb_row13_col3\" class=\"data row13 col3\" >0.41</td>\n",
              "      <td id=\"T_800eb_row13_col4\" class=\"data row13 col4\" >0.45</td>\n",
              "      <td id=\"T_800eb_row13_col5\" class=\"data row13 col5\" >0.44</td>\n",
              "      <td id=\"T_800eb_row13_col6\" class=\"data row13 col6\" >0.39</td>\n",
              "      <td id=\"T_800eb_row13_col7\" class=\"data row13 col7\" >0.36</td>\n",
              "      <td id=\"T_800eb_row13_col8\" class=\"data row13 col8\" >0.35</td>\n",
              "      <td id=\"T_800eb_row13_col9\" class=\"data row13 col9\" >0.35</td>\n",
              "      <td id=\"T_800eb_row13_col10\" class=\"data row13 col10\" >0.54</td>\n",
              "      <td id=\"T_800eb_row13_col11\" class=\"data row13 col11\" >0.56</td>\n",
              "      <td id=\"T_800eb_row13_col12\" class=\"data row13 col12\" >0.44</td>\n",
              "      <td id=\"T_800eb_row13_col13\" class=\"data row13 col13\" >1.00</td>\n",
              "      <td id=\"T_800eb_row13_col14\" class=\"data row13 col14\" >0.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_800eb_level0_row14\" class=\"row_heading level0 row14\" >Total</th>\n",
              "      <td id=\"T_800eb_row14_col0\" class=\"data row14 col0\" >0.18</td>\n",
              "      <td id=\"T_800eb_row14_col1\" class=\"data row14 col1\" >0.52</td>\n",
              "      <td id=\"T_800eb_row14_col2\" class=\"data row14 col2\" >0.53</td>\n",
              "      <td id=\"T_800eb_row14_col3\" class=\"data row14 col3\" >0.49</td>\n",
              "      <td id=\"T_800eb_row14_col4\" class=\"data row14 col4\" >0.55</td>\n",
              "      <td id=\"T_800eb_row14_col5\" class=\"data row14 col5\" >0.69</td>\n",
              "      <td id=\"T_800eb_row14_col6\" class=\"data row14 col6\" >0.71</td>\n",
              "      <td id=\"T_800eb_row14_col7\" class=\"data row14 col7\" >0.70</td>\n",
              "      <td id=\"T_800eb_row14_col8\" class=\"data row14 col8\" >0.65</td>\n",
              "      <td id=\"T_800eb_row14_col9\" class=\"data row14 col9\" >0.68</td>\n",
              "      <td id=\"T_800eb_row14_col10\" class=\"data row14 col10\" >0.74</td>\n",
              "      <td id=\"T_800eb_row14_col11\" class=\"data row14 col11\" >0.80</td>\n",
              "      <td id=\"T_800eb_row14_col12\" class=\"data row14 col12\" >0.73</td>\n",
              "      <td id=\"T_800eb_row14_col13\" class=\"data row14 col13\" >0.65</td>\n",
              "      <td id=\"T_800eb_row14_col14\" class=\"data row14 col14\" >1.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "corr = data.corr()\n",
        "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IR5p8ry44vM9"
      },
      "outputs": [],
      "source": [
        "array = data.values\n",
        "X = array[:,0:14]\n",
        "Y = array[:,14]\n",
        "validation_size = 0.20\n",
        "seed = 42\n",
        "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4-oi-lTVbuI",
        "outputId": "de45387d-38ba-443f-a198-bbf9ee075bb9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((232, 14), (59, 14))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "X_train.shape, X_validation.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qev5cnsvVdRq"
      },
      "outputs": [],
      "source": [
        "num_folds = 10\n",
        "seed = 7\n",
        "scoring = 'accuracy' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPvIn9YoVgaI"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wN-PdhCVe4r"
      },
      "outputs": [],
      "source": [
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXu1hc4bVi9F",
        "outputId": "ec2ce88a-15aa-46a0-e457-0e402b0311a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LR: 0.865761 (0.056587)\n",
            "LDA: 0.939674 (0.055020)\n",
            "KNN: 0.749819 (0.072647)\n",
            "CART: 0.668297 (0.068919)\n",
            "NB: 0.814312 (0.095810)\n",
            "SVM: 0.882971 (0.064815)\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "names = []\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=num_folds, random_state=None)\n",
        "  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "LUHmZzjEVkmC",
        "outputId": "ca6f45a3-54ef-4535-ab42-32082ad2fef6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig = plt.figure(figsize=(10, 10))\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2fLOO08WM46",
        "outputId": "7d6002d3-e603-4a76-95f0-fa0d6ab7f8db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ScaledLR: 0.857246 (0.062071)\n",
            "ScaledLDA: 0.939674 (0.055020)\n",
            "ScaledKNN: 0.763043 (0.047502)\n",
            "ScaledCART: 0.642029 (0.080965)\n",
            "ScaledNB: 0.814312 (0.095810)\n",
            "ScaledSVM: 0.874275 (0.056782)\n"
          ]
        }
      ],
      "source": [
        "pipelines = []\n",
        "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LogisticRegression())])))\n",
        "pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\n",
        "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n",
        "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\n",
        "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n",
        "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\n",
        "results = []\n",
        "names = []\n",
        "for name, model in pipelines:\n",
        "  kfold = KFold(n_splits=num_folds, random_state=None)\n",
        "  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LR\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train)\n",
        "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "penalty = ['none', 'elasticnet', 'l1', 'l2']\n",
        "C = [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "param_grid = dict(solver= solver, penalty = penalty, C = C)\n",
        "model = LogisticRegression()\n",
        "kfold = KFold(n_splits=num_folds, random_state=None)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "grid_result = grid.fit(rescaledX, Y_train)\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4-cfFYdV8Xp",
        "outputId": "6fc4987b-6795-4aba-a8f7-3099ff95aab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.904710 using {'C': 0.001, 'penalty': 'none', 'solver': 'lbfgs'}\n",
            "0.896196 (0.044743) with: {'C': 0.001, 'penalty': 'none', 'solver': 'newton-cg'}\n",
            "0.904710 (0.050778) with: {'C': 0.001, 'penalty': 'none', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 0.001, 'penalty': 'none', 'solver': 'liblinear'}\n",
            "0.887500 (0.055533) with: {'C': 0.001, 'penalty': 'none', 'solver': 'sag'}\n",
            "0.883152 (0.067411) with: {'C': 0.001, 'penalty': 'none', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 0.001, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 0.001, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 0.001, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 0.001, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
            "nan (nan) with: {'C': 0.001, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 0.001, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 0.001, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
            "0.168478 (0.053624) with: {'C': 0.001, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 0.001, 'penalty': 'l1', 'solver': 'sag'}\n",
            "0.280072 (0.128896) with: {'C': 0.001, 'penalty': 'l1', 'solver': 'saga'}\n",
            "0.340942 (0.154285) with: {'C': 0.001, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
            "0.340942 (0.154285) with: {'C': 0.001, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "0.490942 (0.088294) with: {'C': 0.001, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "0.340942 (0.154285) with: {'C': 0.001, 'penalty': 'l2', 'solver': 'sag'}\n",
            "0.340942 (0.154285) with: {'C': 0.001, 'penalty': 'l2', 'solver': 'saga'}\n",
            "0.896196 (0.044743) with: {'C': 0.01, 'penalty': 'none', 'solver': 'newton-cg'}\n",
            "0.904710 (0.050778) with: {'C': 0.01, 'penalty': 'none', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 0.01, 'penalty': 'none', 'solver': 'liblinear'}\n",
            "0.887500 (0.055533) with: {'C': 0.01, 'penalty': 'none', 'solver': 'sag'}\n",
            "0.883152 (0.067411) with: {'C': 0.01, 'penalty': 'none', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
            "nan (nan) with: {'C': 0.01, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 0.01, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 0.01, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
            "0.168478 (0.053624) with: {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 0.01, 'penalty': 'l1', 'solver': 'sag'}\n",
            "0.327899 (0.136852) with: {'C': 0.01, 'penalty': 'l1', 'solver': 'saga'}\n",
            "0.581703 (0.141129) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
            "0.581703 (0.141129) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "0.551812 (0.081888) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "0.581703 (0.141129) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'sag'}\n",
            "0.581703 (0.141129) with: {'C': 0.01, 'penalty': 'l2', 'solver': 'saga'}\n",
            "0.896196 (0.044743) with: {'C': 0.1, 'penalty': 'none', 'solver': 'newton-cg'}\n",
            "0.904710 (0.050778) with: {'C': 0.1, 'penalty': 'none', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 0.1, 'penalty': 'none', 'solver': 'liblinear'}\n",
            "0.887500 (0.055533) with: {'C': 0.1, 'penalty': 'none', 'solver': 'sag'}\n",
            "0.883152 (0.067411) with: {'C': 0.1, 'penalty': 'none', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
            "nan (nan) with: {'C': 0.1, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 0.1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 0.1, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
            "0.530616 (0.123727) with: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 0.1, 'penalty': 'l1', 'solver': 'sag'}\n",
            "0.628986 (0.134649) with: {'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}\n",
            "0.774638 (0.101666) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
            "0.774638 (0.101666) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "0.564674 (0.059082) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "0.774638 (0.101666) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'sag'}\n",
            "0.774638 (0.101666) with: {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}\n",
            "0.896196 (0.044743) with: {'C': 1, 'penalty': 'none', 'solver': 'newton-cg'}\n",
            "0.904710 (0.050778) with: {'C': 1, 'penalty': 'none', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 1, 'penalty': 'none', 'solver': 'liblinear'}\n",
            "0.887500 (0.055533) with: {'C': 1, 'penalty': 'none', 'solver': 'sag'}\n",
            "0.883152 (0.067411) with: {'C': 1, 'penalty': 'none', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 1, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 1, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 1, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 1, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
            "nan (nan) with: {'C': 1, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 1, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 1, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
            "0.650906 (0.069448) with: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 1, 'penalty': 'l1', 'solver': 'sag'}\n",
            "0.883333 (0.055307) with: {'C': 1, 'penalty': 'l1', 'solver': 'saga'}\n",
            "0.861594 (0.054734) with: {'C': 1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
            "0.861594 (0.054734) with: {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "0.620290 (0.068306) with: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "0.861594 (0.054734) with: {'C': 1, 'penalty': 'l2', 'solver': 'sag'}\n",
            "0.861594 (0.054734) with: {'C': 1, 'penalty': 'l2', 'solver': 'saga'}\n",
            "0.896196 (0.044743) with: {'C': 10, 'penalty': 'none', 'solver': 'newton-cg'}\n",
            "0.904710 (0.050778) with: {'C': 10, 'penalty': 'none', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 10, 'penalty': 'none', 'solver': 'liblinear'}\n",
            "0.887500 (0.055533) with: {'C': 10, 'penalty': 'none', 'solver': 'sag'}\n",
            "0.883152 (0.067411) with: {'C': 10, 'penalty': 'none', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 10, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 10, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 10, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 10, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
            "nan (nan) with: {'C': 10, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 10, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 10, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
            "0.727717 (0.096740) with: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 10, 'penalty': 'l1', 'solver': 'sag'}\n",
            "0.883152 (0.067411) with: {'C': 10, 'penalty': 'l1', 'solver': 'saga'}\n",
            "0.883152 (0.058395) with: {'C': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
            "0.883152 (0.058395) with: {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "0.693297 (0.085257) with: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "0.887500 (0.061968) with: {'C': 10, 'penalty': 'l2', 'solver': 'sag'}\n",
            "0.883152 (0.067411) with: {'C': 10, 'penalty': 'l2', 'solver': 'saga'}\n",
            "0.896196 (0.044743) with: {'C': 100, 'penalty': 'none', 'solver': 'newton-cg'}\n",
            "0.904710 (0.050778) with: {'C': 100, 'penalty': 'none', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 100, 'penalty': 'none', 'solver': 'liblinear'}\n",
            "0.887500 (0.055533) with: {'C': 100, 'penalty': 'none', 'solver': 'sag'}\n",
            "0.883152 (0.067411) with: {'C': 100, 'penalty': 'none', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 100, 'penalty': 'elasticnet', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 100, 'penalty': 'elasticnet', 'solver': 'lbfgs'}\n",
            "nan (nan) with: {'C': 100, 'penalty': 'elasticnet', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 100, 'penalty': 'elasticnet', 'solver': 'sag'}\n",
            "nan (nan) with: {'C': 100, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "nan (nan) with: {'C': 100, 'penalty': 'l1', 'solver': 'newton-cg'}\n",
            "nan (nan) with: {'C': 100, 'penalty': 'l1', 'solver': 'lbfgs'}\n",
            "0.719022 (0.097276) with: {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "nan (nan) with: {'C': 100, 'penalty': 'l1', 'solver': 'sag'}\n",
            "0.883152 (0.067411) with: {'C': 100, 'penalty': 'l1', 'solver': 'saga'}\n",
            "0.883333 (0.043871) with: {'C': 100, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
            "0.883333 (0.043871) with: {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "0.723370 (0.099033) with: {'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "0.887500 (0.055533) with: {'C': 100, 'penalty': 'l2', 'solver': 'sag'}\n",
            "0.883152 (0.067411) with: {'C': 100, 'penalty': 'l2', 'solver': 'saga'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "540 fits failed out of a total of 1200.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "60 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 464, in _check_solver\n",
            "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
            "ValueError: penalty='none' is not supported for the liblinear solver\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "60 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "60 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "60 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 459, in _check_solver\n",
            "    solver\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "60 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "60 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1473, in fit\n",
            "    % self.l1_ratio\n",
            "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "60 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "60 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "60 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py\", line 449, in _check_solver\n",
            "    % (solver, penalty)\n",
            "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.89619565 0.90471014        nan 0.8875     0.88315217        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.16847826        nan 0.28007246 0.34094203 0.34094203 0.49094203\n",
            " 0.34094203 0.34094203 0.89619565 0.90471014        nan 0.8875\n",
            " 0.88315217        nan        nan        nan        nan        nan\n",
            "        nan        nan 0.16847826        nan 0.32789855 0.5817029\n",
            " 0.5817029  0.55181159 0.5817029  0.5817029  0.89619565 0.90471014\n",
            "        nan 0.8875     0.88315217        nan        nan        nan\n",
            "        nan        nan        nan        nan 0.53061594        nan\n",
            " 0.62898551 0.77463768 0.77463768 0.56467391 0.77463768 0.77463768\n",
            " 0.89619565 0.90471014        nan 0.8875     0.88315217        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            " 0.6509058         nan 0.88333333 0.8615942  0.8615942  0.62028986\n",
            " 0.8615942  0.8615942  0.89619565 0.90471014        nan 0.8875\n",
            " 0.88315217        nan        nan        nan        nan        nan\n",
            "        nan        nan 0.72771739        nan 0.88315217 0.88315217\n",
            " 0.88315217 0.6932971  0.8875     0.88315217 0.89619565 0.90471014\n",
            "        nan 0.8875     0.88315217        nan        nan        nan\n",
            "        nan        nan        nan        nan 0.71902174        nan\n",
            " 0.88315217 0.88333333 0.88333333 0.72336957 0.8875     0.88315217]\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTtaPOiJWRHa",
        "outputId": "669c5945-91bb-4663-89e6-d94f6584fbb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.948188 using {'shrinkage': 0.05, 'solver': 'lsqr'}\n",
            "nan (nan) with: {'shrinkage': 0.1, 'solver': 'svd'}\n",
            "0.943841 (0.047607) with: {'shrinkage': 0.1, 'solver': 'lsqr'}\n",
            "0.943841 (0.047607) with: {'shrinkage': 0.1, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.2, 'solver': 'svd'}\n",
            "0.943659 (0.051676) with: {'shrinkage': 0.2, 'solver': 'lsqr'}\n",
            "0.943659 (0.051676) with: {'shrinkage': 0.2, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.3, 'solver': 'svd'}\n",
            "0.943659 (0.055213) with: {'shrinkage': 0.3, 'solver': 'lsqr'}\n",
            "0.943659 (0.055213) with: {'shrinkage': 0.3, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.4, 'solver': 'svd'}\n",
            "0.943659 (0.055213) with: {'shrinkage': 0.4, 'solver': 'lsqr'}\n",
            "0.943659 (0.055213) with: {'shrinkage': 0.4, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.5, 'solver': 'svd'}\n",
            "0.922283 (0.046909) with: {'shrinkage': 0.5, 'solver': 'lsqr'}\n",
            "0.922283 (0.046909) with: {'shrinkage': 0.5, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.6, 'solver': 'svd'}\n",
            "0.913587 (0.051610) with: {'shrinkage': 0.6, 'solver': 'lsqr'}\n",
            "0.913587 (0.051610) with: {'shrinkage': 0.6, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.7, 'solver': 'svd'}\n",
            "0.900543 (0.044052) with: {'shrinkage': 0.7, 'solver': 'lsqr'}\n",
            "0.900543 (0.044052) with: {'shrinkage': 0.7, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.8, 'solver': 'svd'}\n",
            "0.870471 (0.034193) with: {'shrinkage': 0.8, 'solver': 'lsqr'}\n",
            "0.870471 (0.034193) with: {'shrinkage': 0.8, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.9, 'solver': 'svd'}\n",
            "0.848732 (0.045379) with: {'shrinkage': 0.9, 'solver': 'lsqr'}\n",
            "0.848732 (0.045379) with: {'shrinkage': 0.9, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.05, 'solver': 'svd'}\n",
            "0.948188 (0.042317) with: {'shrinkage': 0.05, 'solver': 'lsqr'}\n",
            "0.948188 (0.042317) with: {'shrinkage': 0.05, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.25, 'solver': 'svd'}\n",
            "0.943659 (0.055213) with: {'shrinkage': 0.25, 'solver': 'lsqr'}\n",
            "0.943659 (0.055213) with: {'shrinkage': 0.25, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.15, 'solver': 'svd'}\n",
            "0.939493 (0.052004) with: {'shrinkage': 0.15, 'solver': 'lsqr'}\n",
            "0.939493 (0.052004) with: {'shrinkage': 0.15, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.35, 'solver': 'svd'}\n",
            "0.943659 (0.055213) with: {'shrinkage': 0.35, 'solver': 'lsqr'}\n",
            "0.943659 (0.055213) with: {'shrinkage': 0.35, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.45, 'solver': 'svd'}\n",
            "0.926630 (0.047857) with: {'shrinkage': 0.45, 'solver': 'lsqr'}\n",
            "0.926630 (0.047857) with: {'shrinkage': 0.45, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.55, 'solver': 'svd'}\n",
            "0.917935 (0.049507) with: {'shrinkage': 0.55, 'solver': 'lsqr'}\n",
            "0.917935 (0.049507) with: {'shrinkage': 0.55, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.65, 'solver': 'svd'}\n",
            "0.909239 (0.049602) with: {'shrinkage': 0.65, 'solver': 'lsqr'}\n",
            "0.909239 (0.049602) with: {'shrinkage': 0.65, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.75, 'solver': 'svd'}\n",
            "0.874638 (0.041357) with: {'shrinkage': 0.75, 'solver': 'lsqr'}\n",
            "0.874638 (0.041357) with: {'shrinkage': 0.75, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.85, 'solver': 'svd'}\n",
            "0.866123 (0.041536) with: {'shrinkage': 0.85, 'solver': 'lsqr'}\n",
            "0.866123 (0.041536) with: {'shrinkage': 0.85, 'solver': 'eigen'}\n",
            "nan (nan) with: {'shrinkage': 0.95, 'solver': 'svd'}\n",
            "0.844384 (0.040867) with: {'shrinkage': 0.95, 'solver': 'lsqr'}\n",
            "0.844384 (0.040867) with: {'shrinkage': 0.95, 'solver': 'eigen'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "190 fits failed out of a total of 570.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "190 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/discriminant_analysis.py\", line 583, in fit\n",
            "    raise NotImplementedError(\"shrinkage not supported\")\n",
            "NotImplementedError: shrinkage not supported\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [       nan 0.94384058 0.94384058        nan 0.94365942 0.94365942\n",
            "        nan 0.94365942 0.94365942        nan 0.94365942 0.94365942\n",
            "        nan 0.92228261 0.92228261        nan 0.91358696 0.91358696\n",
            "        nan 0.90054348 0.90054348        nan 0.87047101 0.87047101\n",
            "        nan 0.84873188 0.84873188        nan 0.94818841 0.94818841\n",
            "        nan 0.94365942 0.94365942        nan 0.93949275 0.93949275\n",
            "        nan 0.94365942 0.94365942        nan 0.92663043 0.92663043\n",
            "        nan 0.91793478 0.91793478        nan 0.90923913 0.90923913\n",
            "        nan 0.87463768 0.87463768        nan 0.86612319 0.86612319\n",
            "        nan 0.84438406 0.84438406]\n",
            "  category=UserWarning,\n"
          ]
        }
      ],
      "source": [
        "# LDA\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train)\n",
        "shrinkage_values = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.05,0.25,0.15,0.35,0.45,0.55,0.65,0.75, 0.85,0.95]\n",
        "solver_values = ['svd', 'lsqr', 'eigen']\n",
        "param_grid = dict(shrinkage=shrinkage_values, solver = solver_values)\n",
        "model = LinearDiscriminantAnalysis()\n",
        "kfold = KFold(n_splits=num_folds, random_state=None)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "grid_result = grid.fit(rescaledX, Y_train)\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train)\n",
        "param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf']}\n",
        "model = SVC()\n",
        "kfold = KFold(n_splits=num_folds, random_state=None)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "grid_result = grid.fit(rescaledX, Y_train)\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsuLsD15WCZ_",
        "outputId": "63aadda0-fd40-4e13-fad3-2e94a1ea99af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.887500 using {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.327899 (0.136852) with: {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n",
            "0.383877 (0.163434) with: {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "0.327899 (0.136852) with: {'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.327899 (0.136852) with: {'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.327899 (0.136852) with: {'C': 0.1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.362138 (0.133367) with: {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
            "0.865580 (0.060072) with: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "0.840036 (0.055978) with: {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.327899 (0.136852) with: {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.327899 (0.136852) with: {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.379529 (0.136010) with: {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
            "0.870109 (0.061509) with: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "0.882971 (0.055378) with: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.844203 (0.056596) with: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.327899 (0.136852) with: {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.379529 (0.136010) with: {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
            "0.870109 (0.061509) with: {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "0.879348 (0.046344) with: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.882971 (0.055378) with: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.852899 (0.049339) with: {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
            "0.379529 (0.136010) with: {'C': 1000, 'gamma': 1, 'kernel': 'rbf'}\n",
            "0.870109 (0.061509) with: {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "0.887500 (0.056086) with: {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
            "0.883333 (0.051776) with: {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "0.878804 (0.047410) with: {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQ5_6e3FWuWl",
        "outputId": "09ceb912-9718-4637-85b3-5aff3f2d80f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'var_smoothing': 0.0533669923120631}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "nb_classifier = GaussianNB()\n",
        "\n",
        "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
        "gs_NB = GridSearchCV(estimator=nb_classifier, \n",
        "                 param_grid=params_NB, \n",
        "                 verbose=1, \n",
        "                 scoring='accuracy') \n",
        "gs_NB.fit(X_train, Y_train)\n",
        "\n",
        "gs_NB.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG4W3qxJXHIn"
      },
      "source": [
        "## Ensembles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DFa2AJqWv-H",
        "outputId": "448ae74c-9de9-4824-c299-9b28e4a00059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AB: 0.517391 (0.082204)\n",
            "GBM: 0.732428 (0.094565)\n",
            "RF: 0.792391 (0.075146)\n",
            "ET: 0.826630 (0.083168)\n"
          ]
        }
      ],
      "source": [
        "ensembles = []\n",
        "ensembles.append(('AB', AdaBoostClassifier()))\n",
        "ensembles.append(('GBM', GradientBoostingClassifier()))\n",
        "ensembles.append(('RF', RandomForestClassifier()))\n",
        "ensembles.append(('ET', ExtraTreesClassifier()))\n",
        "results = []\n",
        "names = []\n",
        "for name, model in ensembles:\n",
        "  kfold = KFold(n_splits=num_folds, random_state=None)\n",
        "  cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "8bEudl1PXKkP",
        "outputId": "b6c96583-1216-4fa9-fd07-f447daba8a64"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig = pyplot.figure(figsize=(10, 10))\n",
        "fig.suptitle('Scaled Ensemble Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "pyplot.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lHP5vBoXOAV",
        "outputId": "b17ea41f-9755-4854-c439-5687a8f3ad8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.822645 using {'n_estimators': 450}\n",
            "0.788406 (0.086281) with: {'n_estimators': 50}\n",
            "0.801268 (0.096277) with: {'n_estimators': 100}\n",
            "0.788043 (0.086643) with: {'n_estimators': 150}\n",
            "0.783696 (0.076381) with: {'n_estimators': 200}\n",
            "0.783696 (0.071259) with: {'n_estimators': 250}\n",
            "0.805254 (0.081321) with: {'n_estimators': 300}\n",
            "0.813949 (0.087498) with: {'n_estimators': 350}\n",
            "0.813949 (0.078382) with: {'n_estimators': 400}\n",
            "0.818297 (0.067298) with: {'n_estimators': 500}\n",
            "0.822645 (0.086101) with: {'n_estimators': 450}\n",
            "0.818297 (0.082447) with: {'n_estimators': 390}\n",
            "0.818297 (0.086911) with: {'n_estimators': 410}\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train)\n",
        "param_grid = dict(n_estimators=np.array([50, 100, 150, 200, 250, 300, 350, 400, 500, 450, 390, 410]))\n",
        "model = ExtraTreesClassifier(random_state=seed)\n",
        "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "grid_result = grid.fit(rescaledX, Y_train)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KNrXzM9XQeD",
        "outputId": "4ad7b365-ff9f-47b2-a941-6d2e6d9c753c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.809420 using {'n_estimators': 60}\n",
            "0.779167 (0.092878) with: {'n_estimators': 35}\n",
            "0.783696 (0.085710) with: {'n_estimators': 40}\n",
            "0.788225 (0.077423) with: {'n_estimators': 45}\n",
            "0.800906 (0.108498) with: {'n_estimators': 55}\n",
            "0.809420 (0.113842) with: {'n_estimators': 60}\n",
            "0.805072 (0.108862) with: {'n_estimators': 65}\n",
            "0.801268 (0.081001) with: {'n_estimators': 50}\n",
            "0.801268 (0.085541) with: {'n_estimators': 48}\n",
            "0.796739 (0.085113) with: {'n_estimators': 49}\n",
            "0.805435 (0.081470) with: {'n_estimators': 51}\n",
            "0.805435 (0.081470) with: {'n_estimators': 52}\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train)\n",
        "param_grid = dict(n_estimators=np.array([35, 40, 45, 55, 60, 65, 50, 48, 49, 51, 52]))\n",
        "model = RandomForestClassifier(random_state=seed)\n",
        "kfold = KFold(n_splits=num_folds, random_state=None)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "grid_result = grid.fit(rescaledX, Y_train)\n",
        "\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKZ_eaAkXTqz"
      },
      "source": [
        "# Training - Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LG"
      ],
      "metadata": {
        "id": "6rb51r8S10dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelLG= LogisticRegression(C = 0.001, penalty = 'none', solver = 'lbfgs')\n",
        "modelLG.fit(X_train, Y_train)\n",
        "predictions = modelLG.predict(X_validation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfljL5r911sS",
        "outputId": "980d2246-ae9f-4bc9-ed73-33fa8a097557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(Y_train, modelLG.predict(X_train)))\n",
        "print(accuracy_score(Y_validation, predictions))\n",
        "print(confusion_matrix(Y_validation, predictions))\n",
        "print(classification_report(Y_validation, predictions))\n",
        ".8153846153846154"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7T9c4B611jK",
        "outputId": "9511fedc-501b-4243-acec-16461f2432b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.7966101694915254\n",
            "[[ 9  3  0  0  0]\n",
            " [ 0 12  1  0  0]\n",
            " [ 0  4 11  0  0]\n",
            " [ 0  0  0  9  3]\n",
            " [ 0  0  0  1  6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.75      0.86        12\n",
            "         1.0       0.63      0.92      0.75        13\n",
            "         2.0       0.92      0.73      0.81        15\n",
            "         3.0       0.90      0.75      0.82        12\n",
            "         4.0       0.67      0.86      0.75         7\n",
            "\n",
            "    accuracy                           0.80        59\n",
            "   macro avg       0.82      0.80      0.80        59\n",
            "weighted avg       0.84      0.80      0.80        59\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr1S7vC0YqbJ"
      },
      "source": [
        "## GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuG91uQjYIwO"
      },
      "outputs": [],
      "source": [
        "modelNB = GaussianNB(var_smoothing = 0.0533669923120631)\n",
        "modelNB.fit(X_train, Y_train)\n",
        "predictions = modelNB.predict(X_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c3v7kGNYKct",
        "outputId": "db25569f-de0e-4cf8-f08a-509f741c9ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9008620689655172\n",
            "0.8983050847457628\n",
            "[[12  0  0  0  0]\n",
            " [ 0 13  0  0  0]\n",
            " [ 0  0 14  1  0]\n",
            " [ 0  0  2  8  2]\n",
            " [ 0  0  0  1  6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00        12\n",
            "         1.0       1.00      1.00      1.00        13\n",
            "         2.0       0.88      0.93      0.90        15\n",
            "         3.0       0.80      0.67      0.73        12\n",
            "         4.0       0.75      0.86      0.80         7\n",
            "\n",
            "    accuracy                           0.90        59\n",
            "   macro avg       0.89      0.89      0.89        59\n",
            "weighted avg       0.90      0.90      0.90        59\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(accuracy_score(Y_train, modelNB.predict(X_train)))\n",
        "print(accuracy_score(Y_validation, predictions))\n",
        "print(confusion_matrix(Y_validation, predictions))\n",
        "print(classification_report(Y_validation, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "TTK8XOZH1tlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelSVM= SVC(C = 1000, gamma = 0.01, kernel = 'rbf')\n",
        "modelSVM.fit(X_train, Y_train)\n",
        "predictions = modelSVM.predict(X_validation)"
      ],
      "metadata": {
        "id": "PFG-eB1x1wja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(Y_train, modelSVM.predict(X_train)))\n",
        "print(accuracy_score(Y_validation, predictions))\n",
        "print(confusion_matrix(Y_validation, predictions))\n",
        "print(classification_report(Y_validation, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64kXZTkh1x3j",
        "outputId": "c524c304-575c-4774-81db-e482f38853f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.9152542372881356\n",
            "[[12  0  0  0  0]\n",
            " [ 0 13  0  0  0]\n",
            " [ 0  1 13  1  0]\n",
            " [ 0  0  1 11  0]\n",
            " [ 0  0  0  2  5]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00        12\n",
            "         1.0       0.93      1.00      0.96        13\n",
            "         2.0       0.93      0.87      0.90        15\n",
            "         3.0       0.79      0.92      0.85        12\n",
            "         4.0       1.00      0.71      0.83         7\n",
            "\n",
            "    accuracy                           0.92        59\n",
            "   macro avg       0.93      0.90      0.91        59\n",
            "weighted avg       0.92      0.92      0.91        59\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMfViUTAXWwy"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS08v7C6XSYH"
      },
      "outputs": [],
      "source": [
        "modelLDA = LinearDiscriminantAnalysis(shrinkage = 0.05, solver = 'lsqr')\n",
        "modelLDA.fit(X_train, Y_train)\n",
        "predictions = modelLDA.predict(X_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z70jzSK_XaSf",
        "outputId": "f94952ea-34c8-435f-f80f-596f6de26890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9827586206896551\n",
            "0.9152542372881356\n",
            "[[11  1  0  0  0]\n",
            " [ 0 13  0  0  0]\n",
            " [ 0  0 14  1  0]\n",
            " [ 0  0  2 10  0]\n",
            " [ 0  0  0  1  6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.92      0.96        12\n",
            "         1.0       0.93      1.00      0.96        13\n",
            "         2.0       0.88      0.93      0.90        15\n",
            "         3.0       0.83      0.83      0.83        12\n",
            "         4.0       1.00      0.86      0.92         7\n",
            "\n",
            "    accuracy                           0.92        59\n",
            "   macro avg       0.93      0.91      0.92        59\n",
            "weighted avg       0.92      0.92      0.92        59\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(accuracy_score(Y_train, modelLDA.predict(X_train)))\n",
        "print(accuracy_score(Y_validation, predictions))\n",
        "print(confusion_matrix(Y_validation, predictions))\n",
        "print(classification_report(Y_validation, predictions))\n",
        "0.8923076923076924"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(Y_validation, predictions, labels=modelLDA.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=modelLDA.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "uLHHgBaQOVrU",
        "outputId": "312a9f07-c61f-4e4f-a091-dcedcad87abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FP = cm.sum(axis=0) - np.diag(cm)  \n",
        "FN = cm.sum(axis=1) - np.diag(cm)\n",
        "TP = np.diag(cm)\n",
        "#TN= cm.values.sum() - (FP + FN + TP)\n",
        "print(FP)\n",
        "print(FN)\n",
        "print(TP)\n",
        "#print(TN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kRrBxd4QkBK",
        "outputId": "2756ec37-b867-4f2d-b022-61dc274f85fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 2 0]\n",
            "[1 0 1 2 1]\n",
            "[11 13 14 10  6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCYmQKf6XxFB"
      },
      "source": [
        "## ExtraTreesClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UjNP4JwXx3x"
      },
      "outputs": [],
      "source": [
        "modelET = ExtraTreesClassifier(n_estimators = 450)\n",
        "modelET.fit(X_train, Y_train)\n",
        "predictions = modelET.predict(X_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6ViUA9AX1Nj",
        "outputId": "f28b9459-7f08-41e5-f182-d03b7c389451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.864406779661017\n",
            "[[11  1  0  0  0]\n",
            " [ 1 12  0  0  0]\n",
            " [ 0  2 13  0  0]\n",
            " [ 0  0  3  9  0]\n",
            " [ 0  0  1  0  6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.92      0.92        12\n",
            "         1.0       0.80      0.92      0.86        13\n",
            "         2.0       0.76      0.87      0.81        15\n",
            "         3.0       1.00      0.75      0.86        12\n",
            "         4.0       1.00      0.86      0.92         7\n",
            "\n",
            "    accuracy                           0.86        59\n",
            "   macro avg       0.90      0.86      0.87        59\n",
            "weighted avg       0.88      0.86      0.87        59\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(accuracy_score(Y_train, modelET.predict(X_train)))\n",
        "print(accuracy_score(Y_validation, predictions)) \n",
        "print(confusion_matrix(Y_validation, predictions))\n",
        "print(classification_report(Y_validation, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAUG_NHAX37U"
      },
      "source": [
        "## RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ivuCkYUX45i"
      },
      "outputs": [],
      "source": [
        "modelRF = RandomForestClassifier(random_state=seed, n_estimators=60)\n",
        "modelRF.fit(X_train, Y_train)\n",
        "predictions = modelRF.predict(X_validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGp1vYAdX8Lq",
        "outputId": "e11c8300-c117-41eb-d5c7-271dcf9c8dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.864406779661017\n",
            "[[11  1  0  0  0]\n",
            " [ 1 12  0  0  0]\n",
            " [ 0  1 13  1  0]\n",
            " [ 0  0  3  9  0]\n",
            " [ 0  0  1  0  6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.92      0.92      0.92        12\n",
            "         1.0       0.86      0.92      0.89        13\n",
            "         2.0       0.76      0.87      0.81        15\n",
            "         3.0       0.90      0.75      0.82        12\n",
            "         4.0       1.00      0.86      0.92         7\n",
            "\n",
            "    accuracy                           0.86        59\n",
            "   macro avg       0.89      0.86      0.87        59\n",
            "weighted avg       0.87      0.86      0.86        59\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(accuracy_score(Y_train, modelRF.predict(X_train)))\n",
        "print(accuracy_score(Y_validation, predictions))\n",
        "print(confusion_matrix(Y_validation, predictions))\n",
        "print(classification_report(Y_validation, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "YT45y6EzYPAn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvWd7t8ZYFcI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "037a331f-1ce4-4815-f50a-8b7623b59c40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                Họ và tên bạn là gì? Ngày sinh của bạn là?  \\\n",
              "0                             Nguyễn Bình Phương Nhi   2002-08-04 00:00:00   \n",
              "1                                      Trần Thu Hiền   2001-06-01 00:00:00   \n",
              "2                                   PHẠM LÊ MINH ANH            23/10/2001   \n",
              "3                                                NaN            24/07/2001   \n",
              "4  1 cô bé trầm cảm sau môn học từ thất vọng sang...   2001-01-08 00:00:00   \n",
              "\n",
              "  Hiện tại bạn là? Giới tính sinh học của bạn là:  \\\n",
              "0  Sinh viên năm 2                             Nữ   \n",
              "1  Sinh viên năm 3                             Nữ   \n",
              "2  Sinh viên năm 3                             Nữ   \n",
              "3  Sinh viên năm 3                             Nữ   \n",
              "4  Sinh viên năm 3                             Nữ   \n",
              "\n",
              "  Bạn có thuộc cộng đồng LGBT không?   1. Bạn là người có xu hướng:  \\\n",
              "0                              Không  Vừa hướng nội vừa hướng ngoại   \n",
              "1                              Không                    Hướng ngoại   \n",
              "2                              Không                      Hướng nội   \n",
              "3                              Không  Vừa hướng nội vừa hướng ngoại   \n",
              "4                              Không  Vừa hướng nội vừa hướng ngoại   \n",
              "\n",
              "  2. Bạn có cảm thấy bản thân mình thừa thãi không?  \\\n",
              "0                                          Một chút   \n",
              "1                                                Có   \n",
              "2                                                Có   \n",
              "3                                             Không   \n",
              "4                                          Một chút   \n",
              "\n",
              "  3. Bạn có cảm thấy lạc lõng, không hòa nhập được với mọi người không?  \\\n",
              "0                                                 Có                      \n",
              "1                                           Một chút                      \n",
              "2                                                 Có                      \n",
              "3                                           Một chút                      \n",
              "4                                           Một chút                      \n",
              "\n",
              "  4. Bạn đang tự ti? 5. Bạn có hay vận động thể dục, thể thao không?  ...  \\\n",
              "0                 Có                                        Một chút  ...   \n",
              "1                 Có                                        Một chút  ...   \n",
              "2                 Có                                        Một chút  ...   \n",
              "3              Không                                        Một chút  ...   \n",
              "4                 Có                                           Không  ...   \n",
              "\n",
              "  1. Bạn đột nhiên ăn quá nhiều hoặc chán ăn, biếng ăn?  \\\n",
              "0                                                  1      \n",
              "1                                                  3      \n",
              "2                                                  3      \n",
              "3                                                  1      \n",
              "4                                                  2      \n",
              "\n",
              "  2. Bạn luôn thấy mệt mỏi, thiếu năng lượng làm mọi việc?  \\\n",
              "0                                                  1         \n",
              "1                                                  3         \n",
              "2                                                  3         \n",
              "3                                                  2         \n",
              "4                                                  2         \n",
              "\n",
              "  4. Bạn hoạt động chậm chạp, đi lại hay nói chuyện đều chậm khiến mọi người chú ý. Hoặc bồn chồn không thể ở yên một chỗ?  \\\n",
              "0                                                  0                                                                         \n",
              "1                                                  2                                                                         \n",
              "2                                                  3                                                                         \n",
              "3                                                  0                                                                         \n",
              "4                                                  3                                                                         \n",
              "\n",
              "   3. Bạn cảm thấy khó ngủ, ngủ không lâu hoặc ngược lại ngủ quá nhiều?  \\\n",
              "0                                                  0                      \n",
              "1                                                  3                      \n",
              "2                                                  3                      \n",
              "3                                                  3                      \n",
              "4                                                  3                      \n",
              "\n",
              "   5. Bạn không thể tập trung khi làm việc?  \\\n",
              "0                                         1   \n",
              "1                                         2   \n",
              "2                                         3   \n",
              "3                                         1   \n",
              "4                                         3   \n",
              "\n",
              "   7. Bạn luôn cảm thấy chán nản, kiệt sức, tuyệt vọng?  \\\n",
              "0                                                  1      \n",
              "1                                                  3      \n",
              "2                                                  3      \n",
              "3                                                  0      \n",
              "4                                                  3      \n",
              "\n",
              "   6. Bạn mất tự tin vào bản thân, thất vọng về bản thân và cả gia đình?  \\\n",
              "0                                                  2                       \n",
              "1                                                  3                       \n",
              "2                                                  1                       \n",
              "3                                                  1                       \n",
              "4                                                  3                       \n",
              "\n",
              "   8. Bạn ít thấy hứng thú hoặc không tìm thấy niềm vui trong mọi việc?  \\\n",
              "0                                                  2                      \n",
              "1                                                  1                      \n",
              "2                                                  3                      \n",
              "3                                                  0                      \n",
              "4                                                  3                      \n",
              "\n",
              "  9. Bạn có suy nghĩ tiêu cực, muốn làm tổn thương bản thân thậm chí có suy nghĩ tự sát?  \\\n",
              "0                                                  0                                       \n",
              "1                                                  1                                       \n",
              "2                                                  3                                       \n",
              "3                                                  0                                       \n",
              "4                                                  3                                       \n",
              "\n",
              "   Tổng điểm  \n",
              "0          8  \n",
              "1         21  \n",
              "2         25  \n",
              "3          8  \n",
              "4         25  \n",
              "\n",
              "[5 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61f6a859-ecb4-4be3-8870-938657abd7d1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Họ và tên bạn là gì?</th>\n",
              "      <th>Ngày sinh của bạn là?</th>\n",
              "      <th>Hiện tại bạn là?</th>\n",
              "      <th>Giới tính sinh học của bạn là:</th>\n",
              "      <th>Bạn có thuộc cộng đồng LGBT không?</th>\n",
              "      <th>1. Bạn là người có xu hướng:</th>\n",
              "      <th>2. Bạn có cảm thấy bản thân mình thừa thãi không?</th>\n",
              "      <th>3. Bạn có cảm thấy lạc lõng, không hòa nhập được với mọi người không?</th>\n",
              "      <th>4. Bạn đang tự ti?</th>\n",
              "      <th>5. Bạn có hay vận động thể dục, thể thao không?</th>\n",
              "      <th>...</th>\n",
              "      <th>1. Bạn đột nhiên ăn quá nhiều hoặc chán ăn, biếng ăn?</th>\n",
              "      <th>2. Bạn luôn thấy mệt mỏi, thiếu năng lượng làm mọi việc?</th>\n",
              "      <th>4. Bạn hoạt động chậm chạp, đi lại hay nói chuyện đều chậm khiến mọi người chú ý. Hoặc bồn chồn không thể ở yên một chỗ?</th>\n",
              "      <th>3. Bạn cảm thấy khó ngủ, ngủ không lâu hoặc ngược lại ngủ quá nhiều?</th>\n",
              "      <th>5. Bạn không thể tập trung khi làm việc?</th>\n",
              "      <th>7. Bạn luôn cảm thấy chán nản, kiệt sức, tuyệt vọng?</th>\n",
              "      <th>6. Bạn mất tự tin vào bản thân, thất vọng về bản thân và cả gia đình?</th>\n",
              "      <th>8. Bạn ít thấy hứng thú hoặc không tìm thấy niềm vui trong mọi việc?</th>\n",
              "      <th>9. Bạn có suy nghĩ tiêu cực, muốn làm tổn thương bản thân thậm chí có suy nghĩ tự sát?</th>\n",
              "      <th>Tổng điểm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Nguyễn Bình Phương Nhi</td>\n",
              "      <td>2002-08-04 00:00:00</td>\n",
              "      <td>Sinh viên năm 2</td>\n",
              "      <td>Nữ</td>\n",
              "      <td>Không</td>\n",
              "      <td>Vừa hướng nội vừa hướng ngoại</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Có</td>\n",
              "      <td>Có</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Trần Thu Hiền</td>\n",
              "      <td>2001-06-01 00:00:00</td>\n",
              "      <td>Sinh viên năm 3</td>\n",
              "      <td>Nữ</td>\n",
              "      <td>Không</td>\n",
              "      <td>Hướng ngoại</td>\n",
              "      <td>Có</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Có</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PHẠM LÊ MINH ANH</td>\n",
              "      <td>23/10/2001</td>\n",
              "      <td>Sinh viên năm 3</td>\n",
              "      <td>Nữ</td>\n",
              "      <td>Không</td>\n",
              "      <td>Hướng nội</td>\n",
              "      <td>Có</td>\n",
              "      <td>Có</td>\n",
              "      <td>Có</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>24/07/2001</td>\n",
              "      <td>Sinh viên năm 3</td>\n",
              "      <td>Nữ</td>\n",
              "      <td>Không</td>\n",
              "      <td>Vừa hướng nội vừa hướng ngoại</td>\n",
              "      <td>Không</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Không</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1 cô bé trầm cảm sau môn học từ thất vọng sang...</td>\n",
              "      <td>2001-01-08 00:00:00</td>\n",
              "      <td>Sinh viên năm 3</td>\n",
              "      <td>Nữ</td>\n",
              "      <td>Không</td>\n",
              "      <td>Vừa hướng nội vừa hướng ngoại</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Một chút</td>\n",
              "      <td>Có</td>\n",
              "      <td>Không</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61f6a859-ecb4-4be3-8870-938657abd7d1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-61f6a859-ecb4-4be3-8870-938657abd7d1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-61f6a859-ecb4-4be3-8870-938657abd7d1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "test = pd.read_excel('/content/drive/MyDrive/NCKH Depression/Dataset/Education test.xlsx')\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.columns = ['Name', 'DoB', 'Job', 'Sex', 'LGBT', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6', 'a7', 'a8', \n",
        "                'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9', 'b10', 'b11', 'b12', 'b13', 'b14', 'b15',\n",
        "                'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9', 'Total']"
      ],
      "metadata": {
        "id": "CjfZUwPPYU8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change values in total column to the rate of depression. Normal - Minimal - Mild - Medium - Severe.\n",
        "test.Total =test.Total.replace({1: 0, 2: 0, 3: 0, 4: 0,\n",
        "                                 5: 1, 6: 1, 7: 1, 8: 1, 9: 1,\n",
        "                                 10: 2, 11: 2, 12: 2, 13: 2, 14: 2,\n",
        "                                 15: 3, 16: 3, 17: 3, 18: 3, 19: 3,\n",
        "                                 20: 4, 21: 4, 22: 4, 23: 4, 24: 4, 25: 4, 26: 4, 27: 4})\n",
        "\n",
        "# LGBT\n",
        "test.LGBT = test.LGBT.replace({'Có': 1,\n",
        "                               'Không': 0,\n",
        "                               'Chưa xác định': 0.5})\n",
        "\n",
        "test.a2 = test.a2.replace({'Có': 1, 'Một chút': 0.5, 'Không': 0})\n",
        "test.a3 = test.a3.replace({'Có': 1, 'Một chút': 0.5, 'Không': 0})\n",
        "test.a4 = test.a4.replace({'Có': 1, 'Một chút': 0.5, 'Không': 0})\n",
        "#data.a5 = data.a5.replace({'Có': 1, 'Một chút': 0.5, 'Không': 0})\n",
        "#data.a6 = data.a6.replace({'Có': 1, 'Một chút': 0.5, 'Không': 0})\n",
        "\n",
        "# Question (B)\n",
        "#data.b6 = data.b1.replace({'Phần lớn dành cho việc vui chơi': 0, 'Cân đối giữa học tập và vui chơi': 0.5, 'Phần lớn dành cho học tập': 1})\n",
        "test.b15 = test.b15.replace({'Chưa từng': 0, 'Một - hai lần': 1, 'Khoảng năm lần': 2, 'Khoảng 10 lần': 3, 'Rất nhiều': 4, 'Mỗi ngày': 5})\n",
        "\n",
        "# Fill null value with the most popular value.\n",
        "test = test.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
        "\n",
        "# Drop the column Name\n",
        "test = test.drop(columns=[ 'Name', 'DoB', 'a7', 'a8', 'Job', 'Sex', 'a5', 'a6', 'b5', 'b14','b7', 'b2', 'b9', 'b4', 'b1', 'b3', 'b6', 'a1','b10', 'b12', 'a2', 'a4', 'b15'])"
      ],
      "metadata": {
        "id": "WlZSIejmYU5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array = test.values\n",
        "x = array[:,0:14]\n",
        "y = array[:,14]"
      ],
      "metadata": {
        "id": "YGj-Q12oYU1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LG"
      ],
      "metadata": {
        "id": "qZlWmvjb4FaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelLG= LogisticRegression(C = 0.001, penalty = 'none', solver = 'lbfgs')\n",
        "modelLG.fit(X_train, Y_train)\n",
        "predictions = modelLG.predict(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNLqPHnw4J95",
        "outputId": "89f26b20-1e7a-46a8-8c61-c0d060a6b78a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:1484: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  \"Setting penalty='none' will ignore the C and l1_ratio parameters\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y, predictions))\n",
        "print(confusion_matrix(y, predictions))\n",
        "print(classification_report(y, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqAOvPCw4Jne",
        "outputId": "cf226a40-b51e-4a8d-cb83-94bde33f80d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8153846153846154\n",
            "[[ 6  2  0  0  0]\n",
            " [ 2 23  2  0  0]\n",
            " [ 0  2  9  3  0]\n",
            " [ 0  0  0  9  1]\n",
            " [ 0  0  0  0  6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.75      0.75         8\n",
            "         1.0       0.85      0.85      0.85        27\n",
            "         2.0       0.82      0.64      0.72        14\n",
            "         3.0       0.75      0.90      0.82        10\n",
            "         4.0       0.86      1.00      0.92         6\n",
            "\n",
            "    accuracy                           0.82        65\n",
            "   macro avg       0.81      0.83      0.81        65\n",
            "weighted avg       0.82      0.82      0.81        65\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "QmUyI8n331yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelSVM= SVC(C = 1000, gamma = 0.01, kernel = 'rbf')\n",
        "modelSVM.fit(X_train, Y_train)\n",
        "predictions = modelSVM.predict(x)"
      ],
      "metadata": {
        "id": "3ZwWdQqw35VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y, predictions))\n",
        "print(confusion_matrix(y, predictions))\n",
        "print(classification_report(y, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irjg9-Zx36_K",
        "outputId": "686f9903-d85e-4093-d39c-404e5531e29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9384615384615385\n",
            "[[ 8  0  0  0  0]\n",
            " [ 0 27  0  0  0]\n",
            " [ 0  0 12  2  0]\n",
            " [ 0  0  2  8  0]\n",
            " [ 0  0  0  0  6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00         8\n",
            "         1.0       1.00      1.00      1.00        27\n",
            "         2.0       0.86      0.86      0.86        14\n",
            "         3.0       0.80      0.80      0.80        10\n",
            "         4.0       1.00      1.00      1.00         6\n",
            "\n",
            "    accuracy                           0.94        65\n",
            "   macro avg       0.93      0.93      0.93        65\n",
            "weighted avg       0.94      0.94      0.94        65\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA"
      ],
      "metadata": {
        "id": "MaW9I529ZwZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "metadata": {
        "id": "zF1S3qSaaEjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelLDA = LinearDiscriminantAnalysis(shrinkage = 0.05, solver = 'lsqr')\n",
        "\n",
        "modelLDA.fit(X_train, Y_train) # Fit model vào tệp train\n",
        "predictions = modelLDA.predict(x) # Đưa array input X vào method predict() để dự đoán ra output kết quả"
      ],
      "metadata": {
        "id": "HsDvyavAZjmY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "3ed183bb-a4ba-4d80-ce20-050aaadc9bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0dcdc80c060e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodelLDA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshrinkage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'lsqr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodelLDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Fit model vào tệp train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelLDA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Đưa array input X vào method predict() để dự đoán ra output kết quả\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y, predictions))\n",
        "print(confusion_matrix(y, predictions))\n",
        "print(classification_report(y, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-7R6TuDZ8Mp",
        "outputId": "6d8a7cab-0751-4634-852c-74b77020c7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8923076923076924\n",
            "[[ 7  1  0  0  0]\n",
            " [ 0 27  0  0  0]\n",
            " [ 0  4  8  2  0]\n",
            " [ 0  0  0 10  0]\n",
            " [ 0  0  0  0  6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.88      0.93         8\n",
            "         1.0       0.84      1.00      0.92        27\n",
            "         2.0       1.00      0.57      0.73        14\n",
            "         3.0       0.83      1.00      0.91        10\n",
            "         4.0       1.00      1.00      1.00         6\n",
            "\n",
            "    accuracy                           0.89        65\n",
            "   macro avg       0.94      0.89      0.90        65\n",
            "weighted avg       0.91      0.89      0.88        65\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y, predictions, labels=modelLDA.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=modelLDA.classes_)\n",
        "t=disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "TLjuq_jbs8Df",
        "outputId": "08effa92-b786-457b-d58e-fbc36996f738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfm_plot = plot_confusion_matrix(<estimator>, <X>, <Y>)\n",
        "cfm_plot.savefig(\"cfm.png\")"
      ],
      "metadata": {
        "id": "0UBK0Q_mvbOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NB"
      ],
      "metadata": {
        "id": "G7pRLNQrZZad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelNB = GaussianNB(var_smoothing = 0.0533669923120631)\n",
        "modelNB.fit(X_train, Y_train)\n",
        "predictions = modelNB.predict(x)"
      ],
      "metadata": {
        "id": "1rEOBnhDZNvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y, predictions))\n",
        "print(confusion_matrix(y, predictions))\n",
        "print(classification_report(y, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1VBDx8ZZc-V",
        "outputId": "92f2ddef-89fd-417f-f2a3-b3687d37cc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7230769230769231\n",
            "[[ 6  2  0  0  0]\n",
            " [ 0 22  5  0  0]\n",
            " [ 0  3  7  4  0]\n",
            " [ 0  0  3  7  0]\n",
            " [ 0  0  0  1  5]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.75      0.86         8\n",
            "         1.0       0.81      0.81      0.81        27\n",
            "         2.0       0.47      0.50      0.48        14\n",
            "         3.0       0.58      0.70      0.64        10\n",
            "         4.0       1.00      0.83      0.91         6\n",
            "\n",
            "    accuracy                           0.72        65\n",
            "   macro avg       0.77      0.72      0.74        65\n",
            "weighted avg       0.74      0.72      0.73        65\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ET"
      ],
      "metadata": {
        "id": "O76SQjeFaV8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelET = ExtraTreesClassifier(n_estimators = 450)\n",
        "modelET.fit(X_train, Y_train)\n",
        "predictions = modelET.predict(x)"
      ],
      "metadata": {
        "id": "h9ShN78tZ8KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y, predictions)) \n",
        "print(confusion_matrix(y, predictions))\n",
        "print(classification_report(y, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icpfhUpOZ8G1",
        "outputId": "38c19e79-b695-475a-ecbd-8fa44307cd1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.676923076923077\n",
            "[[ 6  2  0  0  0]\n",
            " [ 1 21  5  0  0]\n",
            " [ 0  3  6  5  0]\n",
            " [ 0  0  4  5  1]\n",
            " [ 0  0  0  0  6]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.75      0.80         8\n",
            "         1.0       0.81      0.78      0.79        27\n",
            "         2.0       0.40      0.43      0.41        14\n",
            "         3.0       0.50      0.50      0.50        10\n",
            "         4.0       0.86      1.00      0.92         6\n",
            "\n",
            "    accuracy                           0.68        65\n",
            "   macro avg       0.68      0.69      0.69        65\n",
            "weighted avg       0.68      0.68      0.68        65\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RF"
      ],
      "metadata": {
        "id": "23xx8LfMa29b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "modelRF = RandomForestClassifier(random_state=seed, n_estimators=60)\n",
        "modelRF.fit(X_train, Y_train)\n",
        "predictions = modelRF.predict(x)"
      ],
      "metadata": {
        "id": "sgoFCt1RZ8Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(y, predictions))\n",
        "print(confusion_matrix(y, predictions))\n",
        "print(classification_report(y, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l_D9j8iZ8Ax",
        "outputId": "d2c392c3-0ee5-4017-f35c-7c213f7ea980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6615384615384615\n",
            "[[ 6  2  0  0  0]\n",
            " [ 1 23  3  0  0]\n",
            " [ 0  4  3  7  0]\n",
            " [ 0  0  3  6  1]\n",
            " [ 0  0  0  1  5]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.75      0.80         8\n",
            "         1.0       0.79      0.85      0.82        27\n",
            "         2.0       0.33      0.21      0.26        14\n",
            "         3.0       0.43      0.60      0.50        10\n",
            "         4.0       0.83      0.83      0.83         6\n",
            "\n",
            "    accuracy                           0.66        65\n",
            "   macro avg       0.65      0.65      0.64        65\n",
            "weighted avg       0.65      0.66      0.65        65\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zn5Kf16aZ750"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qG4W3qxJXHIn",
        "6rb51r8S10dt",
        "rr1S7vC0YqbJ",
        "TTK8XOZH1tlW",
        "wMfViUTAXWwy",
        "CCYmQKf6XxFB",
        "MAUG_NHAX37U",
        "qZlWmvjb4FaQ",
        "QmUyI8n331yR",
        "G7pRLNQrZZad",
        "O76SQjeFaV8m",
        "23xx8LfMa29b"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}